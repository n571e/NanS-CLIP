# -*- coding: utf-8 -*-
"""
Gradio å—å®‹æ–‡åŒ–å¤šæ¨¡æ€æ£€ç´¢ Demoã€‚

ä½¿ç”¨æ–¹æ³•:
    conda activate pytorch
    cd d:/Desktop/CV/CLIP/NanS-CLIP

    # å®‰è£… gradioï¼ˆå¦‚æœå°šæœªå®‰è£…ï¼‰
    pip install gradio

    # è¿è¡Œ Demo
    python demo.py

    # ä½¿ç”¨ LoRA å¾®è°ƒåçš„æ¨¡å‹
    python demo.py --lora_path ../clip_data/experiments/lora_song/best_lora.pt
"""

import os
import json
import argparse
from pathlib import Path

import torch
from PIL import Image

from cn_clip.clip import load_from_name, tokenize
from cn_clip.clip.lora import inject_lora, load_lora_state_dict


def load_image_database(image_dir: str, annotations_path: str):
    """åŠ è½½å›¾ç‰‡æ•°æ®åº“"""
    images = []
    image_paths = []
    titles = []

    # å¦‚æœæœ‰æ ‡æ³¨æ–‡ä»¶ï¼Œç”¨æ ‡æ³¨æ–‡ä»¶ä¸­çš„å›¾ç‰‡
    if os.path.isfile(annotations_path):
        with open(annotations_path, "r", encoding="utf-8") as f:
            annotations = json.load(f)
        for ann in annotations:
            img_path = os.path.join(image_dir, ann["filename"])
            if os.path.isfile(img_path):
                image_paths.append(img_path)
                titles.append(ann.get("title", ann["filename"]))
    else:
        # ç›´æ¥æ‰«æç›®å½•
        exts = {".jpg", ".jpeg", ".png", ".webp"}
        for p in sorted(Path(image_dir).iterdir()):
            if p.suffix.lower() in exts:
                image_paths.append(str(p))
                titles.append(p.stem)

    return image_paths, titles


def build_image_features(model, preprocess, image_paths, device):
    """é¢„è®¡ç®—æ‰€æœ‰å›¾ç‰‡çš„ç‰¹å¾å‘é‡"""
    features = []
    for path in image_paths:
        img = preprocess(Image.open(path).convert("RGB")).unsqueeze(0).to(device)
        with torch.no_grad():
            with torch.amp.autocast("cuda"):
                feat = model.encode_image(img)
                feat = feat / feat.norm(dim=-1, keepdim=True)
        features.append(feat.cpu())
    return torch.cat(features, dim=0)


def main():
    import sys
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', line_buffering=True)
    parser = argparse.ArgumentParser(description="å—å®‹æ–‡åŒ–å¤šæ¨¡æ€æ£€ç´¢ Demo")
    parser.add_argument("--image_dir", type=str, default="data/images")
    parser.add_argument("--annotations", type=str, default="data/annotations.json")
    parser.add_argument("--pretrained", type=str, default="../clip_data/pretrained_weights/")
    parser.add_argument("--lora_path", type=str, default=None)
    parser.add_argument("--rank", type=int, default=4)
    parser.add_argument("--alpha", type=float, default=16.0)
    parser.add_argument("--port", type=int, default=7860)
    args = parser.parse_args()

    # å»¶è¿Ÿå¯¼å…¥ gradioï¼ˆè¿™æ · --help ä¸éœ€è¦å®‰è£… gradioï¼‰
    try:
        import gradio as gr
    except ImportError:
        print("âŒ ç¼ºå°‘ gradioï¼Œæ­£åœ¨å®‰è£…...")
        os.system("pip install gradio")
        import gradio as gr

    device = "cuda" if torch.cuda.is_available() else "cpu"

    # åŠ è½½æ¨¡å‹
    print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
    model, preprocess = load_from_name("ViT-B-16", device="cpu", download_root=args.pretrained)

    mode_label = "Zero-Shot"
    if args.lora_path and os.path.isfile(args.lora_path):
        inject_lora(model, rank=args.rank, alpha=args.alpha)
        state_dict = torch.load(args.lora_path, map_location="cpu")
        load_lora_state_dict(model, state_dict)
        mode_label = "LoRA å¾®è°ƒ"
        print(f"   LoRA å·²åŠ è½½: {args.lora_path}")

    model = model.to(device)
    model.eval()

    # åŠ è½½å›¾ç‰‡æ•°æ®åº“
    print("ğŸ–¼ï¸ åŠ è½½å›¾ç‰‡æ•°æ®åº“...")
    image_paths, titles = load_image_database(args.image_dir, args.annotations)
    print(f"   å…± {len(image_paths)} å¼ å›¾ç‰‡")

    if len(image_paths) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ï¼Œè¯·ç¡®è®¤ data/images/ ç›®å½•")
        return

    # é¢„è®¡ç®—å›¾ç‰‡ç‰¹å¾
    print("ğŸ”§ é¢„è®¡ç®—å›¾ç‰‡ç‰¹å¾...")
    image_features = build_image_features(model, preprocess, image_paths, device)
    print("   å®Œæˆï¼")

    # ---- Gradio ç•Œé¢ ----
    def text_to_image_search(query_text, top_k=5):
        """æ–‡æœ¬æŸ¥è¯¢ â†’ æ£€ç´¢æœ€ç›¸ä¼¼å›¾ç‰‡"""
        if not query_text.strip():
            return []

        tokens = tokenize([query_text]).to(device)
        with torch.no_grad():
            with torch.amp.autocast("cuda"):
                text_feat = model.encode_text(tokens)
                text_feat = text_feat / text_feat.norm(dim=-1, keepdim=True)

        sims = (text_feat.cpu() @ image_features.T).squeeze()
        topk = sims.topk(min(top_k, len(image_paths)))

        results = []
        for score, idx in zip(topk.values, topk.indices):
            img = Image.open(image_paths[idx]).convert("RGB")
            label = f"{titles[idx]} (ç›¸ä¼¼åº¦: {score:.3f})"
            results.append((img, label))
        return results

    def image_to_text_search(query_image, candidate_texts):
        """å›¾ç‰‡ â†’ æ£€ç´¢æœ€åŒ¹é…çš„æ–‡æœ¬"""
        if query_image is None or not candidate_texts.strip():
            return "è¯·ä¸Šä¼ å›¾ç‰‡å¹¶è¾“å…¥å€™é€‰æ–‡æœ¬"

        img = preprocess(Image.fromarray(query_image).convert("RGB")).unsqueeze(0).to(device)
        with torch.no_grad():
            with torch.amp.autocast("cuda"):
                img_feat = model.encode_image(img)
                img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)

        text_list = [t.strip() for t in candidate_texts.split("\n") if t.strip()]
        tokens = tokenize(text_list).to(device)
        with torch.no_grad():
            with torch.amp.autocast("cuda"):
                txt_feats = model.encode_text(tokens)
                txt_feats = txt_feats / txt_feats.norm(dim=-1, keepdim=True)

        sims = (img_feat.cpu() @ txt_feats.cpu().T).squeeze()
        sorted_indices = sims.argsort(descending=True)

        results = []
        for idx in sorted_indices:
            results.append(f"  {sims[idx]:.3f}  |  {text_list[idx]}")
        return "\n".join(results)

    # æ„å»ºç•Œé¢
    with gr.Blocks(title="å—å®‹æ–‡åŒ–å¤šæ¨¡æ€æ£€ç´¢") as demo:
        # ä¸ºäº†å…¼å®¹ Gradio 6.0ï¼Œå¦‚æœæƒ³è®¾ç½®ä¸»é¢˜ï¼Œå¯ä»¥ç›´æ¥åœ¨ launch é‡Œæ”¹
        gr.Markdown(f"""
        # ğŸ¯ å—å®‹æ–‡åŒ–å¤šæ¨¡æ€æ£€ç´¢ç³»ç»Ÿ
        **æ¨¡å‹**: Chinese-CLIP ViT-B-16 ({mode_label}) | **å›¾ç‰‡åº“**: {len(image_paths)} å¼ 
        """)

        with gr.Tab("ğŸ“ æ–‡å­—æœå›¾"):
            gr.Markdown("è¾“å…¥ä¸­æ–‡æè¿°æˆ–å¤æ–‡ï¼Œæ£€ç´¢æœ€ç›¸å…³çš„å›¾ç‰‡")
            with gr.Row():
                text_input = gr.Textbox(
                    label="è¾“å…¥æŸ¥è¯¢æ–‡æœ¬",
                    placeholder="ä¾‹ï¼šè¥¿æ¹–ç¾æ™¯ã€å—å®‹å¤ç”»ã€é’ç“·ç¢—",
                    lines=2,
                )
                top_k_slider = gr.Slider(1, 10, value=5, step=1, label="è¿”å›æ•°é‡")
            text_btn = gr.Button("ğŸ” æœç´¢", variant="primary")
            gallery = gr.Gallery(label="æ£€ç´¢ç»“æœ", columns=5, height=400)
            text_btn.click(text_to_image_search, [text_input, top_k_slider], gallery)

            gr.Markdown("### ğŸ’¡ è¯•è¯•è¿™äº›æŸ¥è¯¢")
            gr.Examples(
                [["è¥¿æ¹–ç¾æ™¯"], ["å—å®‹å±±æ°´ç”»"], ["é’ç“·"], ["å®«æ®¿å»ºç­‘"], ["å¾·å¯¿å®«"]],
                inputs=[text_input],
            )

        with gr.Tab("ğŸ–¼ï¸ ä»¥å›¾æœæ–‡"):
            gr.Markdown("ä¸Šä¼ å›¾ç‰‡ï¼Œåœ¨å€™é€‰æ–‡æœ¬ä¸­æ‰¾æœ€åŒ¹é…çš„æè¿°")
            with gr.Row():
                img_input = gr.Image(label="ä¸Šä¼ å›¾ç‰‡")
                txt_candidates = gr.Textbox(
                    label="å€™é€‰æ–‡æœ¬ï¼ˆæ¯è¡Œä¸€æ¡ï¼‰",
                    placeholder="è¥¿æ¹–ç¾æ™¯\nå—å®‹å¤ç”»\nå®‹ä»£é’ç“·\nå±±æ°´ç”»å·",
                    lines=6,
                )
            img_btn = gr.Button("ğŸ” åŒ¹é…", variant="primary")
            match_result = gr.Textbox(label="åŒ¹é…ç»“æœï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰", lines=8)
            img_btn.click(image_to_text_search, [img_input, txt_candidates], match_result)

    print(f"\nğŸš€ å¯åŠ¨ Demo: http://localhost:{args.port}")
    demo.launch(server_port=args.port, share=False)


if __name__ == "__main__":
    main()
