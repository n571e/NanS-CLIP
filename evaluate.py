# -*- coding: utf-8 -*-
"""
è¯„ä¼° Chinese-CLIP çš„æ£€ç´¢æ€§èƒ½ï¼šZero-Shot vs LoRA å¾®è°ƒ å¯¹æ¯”ã€‚
è®¡ç®— Textâ†’Image å’Œ Imageâ†’Text çš„ Recall@Kã€‚

ä½¿ç”¨æ–¹æ³•:
    conda activate pytorch
    cd d:/Desktop/CV/CLIP/NanS-CLIP

    # è¯„ä¼° Zero-Shot åŸºçº¿
    python evaluate.py --mode zeroshot

    # è¯„ä¼° LoRA å¾®è°ƒå
    python evaluate.py --mode lora --lora_path ../clip_data/experiments/lora_song/best_lora.pt

    # Hard Negative è¯„æµ‹ï¼ˆæ··å…¥å¹²æ‰°å›¾ç‰‡ï¼‰
    python evaluate.py --distractor_dir data/distractors
"""

import os
import json
import argparse
import base64
from io import BytesIO
from pathlib import Path

import torch
import lmdb
import pickle
from tqdm import tqdm
from PIL import Image

from cn_clip.clip import load_from_name, tokenize
from cn_clip.clip.lora import inject_lora, load_lora_state_dict


def load_eval_data(lmdb_dir: str, preprocess, max_txt_length: int = 52):
    """ä» LMDB åŠ è½½è¯„ä¼°æ•°æ®ï¼ˆä¸é‡å¤çš„å›¾ç‰‡å’Œæ–‡æœ¬ï¼‰"""
    # è¯»å–æ‰€æœ‰ pairs
    env_pairs = lmdb.open(os.path.join(lmdb_dir, "pairs"), readonly=True, lock=False)
    env_imgs = lmdb.open(os.path.join(lmdb_dir, "imgs"), readonly=True, lock=False)

    with env_pairs.begin() as txn:
        num = int(txn.get(b"num_samples").decode("utf-8"))

    # æ”¶é›†æ‰€æœ‰å›¾æ–‡å¯¹
    pairs = []
    with env_pairs.begin() as txn:
        for i in range(num):
            data = pickle.loads(txn.get(str(i).encode("utf-8")))
            image_id, text_id, text = data
            pairs.append((image_id, text))

    # æå–å”¯ä¸€å›¾ç‰‡
    unique_image_ids = sorted(set(p[0] for p in pairs))
    images = []
    with env_imgs.begin() as txn:
        for img_id in unique_image_ids:
            b64 = txn.get(str(img_id).encode("utf-8"))
            if b64:
                img_bytes = base64.b64decode(b64.decode("utf-8"))
                img = Image.open(BytesIO(img_bytes)).convert("RGB")
                images.append((img_id, preprocess(img)))

    # æ„å»º image_id â†’ æ•°ç»„ä½ç½® çš„æ˜ å°„
    imgid_to_pos = {iid: pos for pos, (iid, _) in enumerate(images)}

    # æå–å”¯ä¸€æ–‡æœ¬
    unique_texts = list(set(p[1] for p in pairs))

    # æ„å»º ground truth æ˜ å°„ï¼ˆç”¨æ•°ç»„ä½ç½®ï¼Œä¸ç”¨ image_idï¼‰
    # text_idx -> set of å›¾ç‰‡æ•°ç»„ä½ç½®
    text_to_images = {}
    # img_pos -> set of æ–‡æœ¬æ•°ç»„ä½ç½®
    image_to_texts = {}
    for img_id, text in pairs:
        text_idx = unique_texts.index(text)
        img_pos = imgid_to_pos.get(img_id)
        if img_pos is None:
            continue

        if text_idx not in text_to_images:
            text_to_images[text_idx] = set()
        text_to_images[text_idx].add(img_pos)

        if img_pos not in image_to_texts:
            image_to_texts[img_pos] = set()
        image_to_texts[img_pos].add(text_idx)

    env_pairs.close()
    env_imgs.close()

    return images, unique_texts, text_to_images, image_to_texts


def load_distractors(distractor_dir: str, preprocess, start_id: int = 100000):
    """åŠ è½½å¹²æ‰°å›¾ç‰‡ï¼ˆHard Negativeï¼‰ã€‚
    
    å¹²æ‰°å›¾ç‰‡ä½¿ç”¨ä¸è®­ç»ƒæ•°æ®ä¸é‡å çš„ image_idï¼ˆä» start_id å¼€å§‹ï¼‰ï¼Œ
    ä»…æ··å…¥å›¾ç‰‡æ£€ç´¢æ± ï¼Œä¸å½±å“ ground truth æ˜ å°„ã€‚
    """
    distractor_dir = Path(distractor_dir)
    if not distractor_dir.exists():
        print(f"   âš ï¸ å¹²æ‰°å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {distractor_dir}")
        return []

    exts = {".jpg", ".jpeg", ".png", ".webp"}
    distractors = []
    for i, p in enumerate(sorted(distractor_dir.iterdir())):
        if p.suffix.lower() not in exts or not p.is_file():
            continue
        try:
            img = Image.open(p).convert("RGB")
            distractors.append((start_id + i, preprocess(img)))
        except Exception:
            continue
    return distractors


def compute_features(model, images, texts, device, batch_size=32):
    """æå–æ‰€æœ‰å›¾ç‰‡å’Œæ–‡æœ¬çš„ç‰¹å¾å‘é‡"""
    model.eval()

    # å›¾ç‰‡ç‰¹å¾
    all_img_feats = []
    for i in range(0, len(images), batch_size):
        batch = torch.stack([img for _, img in images[i:i+batch_size]]).to(device)
        with torch.no_grad():
            with torch.amp.autocast("cuda"):
                feats = model.encode_image(batch)
                feats = feats / feats.norm(dim=-1, keepdim=True)
        all_img_feats.append(feats.cpu())
    image_features = torch.cat(all_img_feats, dim=0)

    # æ–‡æœ¬ç‰¹å¾
    all_txt_feats = []
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        tokens = tokenize(batch_texts, context_length=52).to(device)
        with torch.no_grad():
            with torch.amp.autocast("cuda"):
                feats = model.encode_text(tokens)
                feats = feats / feats.norm(dim=-1, keepdim=True)
        all_txt_feats.append(feats.cpu())
    text_features = torch.cat(all_txt_feats, dim=0)

    return image_features, text_features


def recall_at_k(sim_matrix, ground_truth, k_list=[1, 5, 10]):
    """è®¡ç®— Recall@K å’Œ Mean Recall"""
    results = {}
    for k in k_list:
        correct = 0
        total = 0
        for i in range(sim_matrix.shape[0]):
            if i not in ground_truth:
                continue
            topk = sim_matrix[i].topk(k).indices.tolist()
            gt = ground_truth[i]
            if any(t in gt for t in topk):
                correct += 1
            total += 1
        results[f"R@{k}"] = correct / max(total, 1) * 100
    results["MR"] = sum(results.values()) / len(results)
    return results


def main():
    import sys
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    parser = argparse.ArgumentParser(description="Chinese-CLIP æ£€ç´¢è¯„ä¼°")
    parser.add_argument("--mode", choices=["zeroshot", "lora", "both"], default="both")
    parser.add_argument("--lora_path", type=str, default="../clip_data/experiments/lora_song/best_lora.pt")
    parser.add_argument("--data_dir", type=str, default="../clip_data/datasets/SongDynasty/lmdb/valid")
    parser.add_argument("--pretrained", type=str, default="../clip_data/pretrained_weights/")
    parser.add_argument("--rank", type=int, default=8)
    parser.add_argument("--alpha", type=float, default=16.0)
    parser.add_argument("--distractor_dir", type=str, default="",
                        help="å¹²æ‰°å›¾ç‰‡ç›®å½•ï¼ˆHard Negative è¯„æµ‹ï¼‰ï¼Œå¦‚ data/distractors")
    args = parser.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"

    # åŠ è½½æ•°æ®ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰
    print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
    model, preprocess = load_from_name("ViT-B-16", device="cpu", download_root=args.pretrained)
    model.float()

    print("ğŸ“‚ åŠ è½½è¯„ä¼°æ•°æ®...")
    images, texts, text_to_images, image_to_texts = load_eval_data(args.data_dir, preprocess)
    num_domain_images = len(images)
    print(f"   é¢†åŸŸå›¾ç‰‡: {num_domain_images} | æ–‡æœ¬: {len(texts)}")

    # åŠ è½½å¹²æ‰°å›¾ç‰‡ï¼ˆHard Negativeï¼‰
    if args.distractor_dir:
        distractors = load_distractors(args.distractor_dir, preprocess)
        if distractors:
            images = images + distractors  # è¿½åŠ åˆ°æœ«å°¾ï¼Œä¸å½±å“ ground truth ä½ç½®
            print(f"   ğŸ¯ å¹²æ‰°å›¾ç‰‡: {len(distractors)} | æ€»æ£€ç´¢æ± : {len(images)}")

    modes = [args.mode] if args.mode != "both" else ["zeroshot", "lora"]
    all_results = {}

    for mode in modes:
        print(f"\n{'='*50}")
        print(f"  è¯„ä¼°æ¨¡å¼: {mode}")
        print(f"{'='*50}")

        if mode == "lora":
            # é‡æ–°åŠ è½½å¹²å‡€æ¨¡å‹å†æ³¨å…¥ LoRA
            model, preprocess = load_from_name("ViT-B-16", device="cpu", download_root=args.pretrained)
            model.float()
            inject_lora(model, rank=args.rank, alpha=args.alpha)
            if os.path.isfile(args.lora_path):
                state_dict = torch.load(args.lora_path, map_location="cpu", weights_only=False)
                load_lora_state_dict(model, state_dict)
                print(f"   LoRA æƒé‡å·²åŠ è½½: {args.lora_path}")
            else:
                print(f"   âš ï¸ LoRA æƒé‡ä¸å­˜åœ¨: {args.lora_path}ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")

        model = model.to(device)
        model.eval()

        print("ğŸ”§ æå–ç‰¹å¾...")
        image_features, text_features = compute_features(model, images, texts, device)

        sim_t2i = text_features @ image_features.T
        sim_i2t = image_features @ text_features.T

        t2i_recall = recall_at_k(sim_t2i, text_to_images)
        i2t_recall = recall_at_k(sim_i2t, image_to_texts)

        print(f"\n  Text â†’ Image:")
        for k, v in t2i_recall.items():
            print(f"    {k}: {v:.1f}%")
        print(f"\n  Image â†’ Text:")
        for k, v in i2t_recall.items():
            print(f"    {k}: {v:.1f}%")

        all_results[mode] = {
            "text_to_image": t2i_recall,
            "image_to_text": i2t_recall,
        }

        result = {"mode": mode, **all_results[mode],
                  "num_domain_images": num_domain_images,
                  "num_distractors": len(images) - num_domain_images,
                  "num_total_images": len(images),
                  "num_texts": len(texts)}
        with open(Path(f"eval_results_{mode}.json"), "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

    # å¦‚æœè·‘äº† both æ¨¡å¼ï¼Œæ‰“å°å¯¹æ¯”è¡¨æ ¼
    if len(all_results) == 2:
        pool_desc = f"ï¼ˆæ£€ç´¢æ± : {len(images)} å¼ ï¼Œå…¶ä¸­å¹²æ‰° {len(images) - num_domain_images}ï¼‰" if len(images) > num_domain_images else ""
        print(f"\n{'='*60}")
        print(f"  ğŸ“Š Zero-Shot vs LoRA å¯¹æ¯” {pool_desc}")
        print(f"{'='*60}")
        print(f"  {'æŒ‡æ ‡':<16} {'Zero-Shot':>10} {'LoRA':>10} {'æå‡':>10}")
        print(f"  {'-'*50}")
        zs = all_results["zeroshot"]
        lo = all_results["lora"]
        for direction, label in [("text_to_image", "Tâ†’I"), ("image_to_text", "Iâ†’T")]:
            for k in ["R@1", "R@5", "R@10", "MR"]:
                z = zs[direction][k]
                l = lo[direction][k]
                delta = l - z
                sign = "+" if delta >= 0 else ""
                print(f"  {label} {k:<11} {z:>9.1f}% {l:>9.1f}% {sign}{delta:>8.1f}%")
        print(f"  {'='*50}")

    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜")


if __name__ == "__main__":
    main()
