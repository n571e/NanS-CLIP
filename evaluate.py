# -*- coding: utf-8 -*-
"""
è¯„ä¼° Chinese-CLIP çš„æ£€ç´¢æ€§èƒ½ï¼šZero-Shot vs LoRA å¾®è°ƒ å¯¹æ¯”ã€‚
è®¡ç®— Textâ†’Image å’Œ Imageâ†’Text çš„ Recall@Kã€‚

ä½¿ç”¨æ–¹æ³•:
    conda activate pytorch
    cd d:/Desktop/CV/CLIP/NanS-CLIP

    # è¯„ä¼° Zero-Shot åŸºçº¿
    python evaluate.py --mode zeroshot

    # è¯„ä¼° LoRA å¾®è°ƒå
    python evaluate.py --mode lora --lora_path ../clip_data/experiments/lora_song/best_lora.pt

    # Hard Negative è¯„æµ‹ï¼ˆæ··å…¥å¹²æ‰°å›¾ç‰‡ï¼‰
    python evaluate.py --distractor_dir data/distractors
"""

import os
import json
import base64
import argparse
from pathlib import Path
from io import BytesIO
import pickle
import math
import logging

import lmdb
import torch
import numpy as np
from tqdm import tqdm
from PIL import Image

# ============ æ—¥å¿—é…ç½® ============
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

from cn_clip.clip import load_from_name, tokenize
from cn_clip.clip.lora import inject_lora, load_lora_state_dict


def load_eval_data(lmdb_dir: str, preprocess):
    """ä» LMDB åŠ è½½æ‰€æœ‰å¾…è¯„æµ‹æ•°æ®å’Œ Ground Truth"""
    lmdb_dir = Path(lmdb_dir)
    env_imgs = lmdb.open(str(lmdb_dir / "imgs"), readonly=True, lock=False)
    env_pairs = lmdb.open(str(lmdb_dir / "pairs"), readonly=True, lock=False)

    # æ”¶é›†æ‰€æœ‰å›¾æ–‡å¯¹
    pairs = []
    with env_pairs.begin() as txn:
        num_pairs = txn.stat()["entries"]
        for i in range(num_pairs):
            data = pickle.loads(txn.get(str(i).encode("utf-8")))
            image_id, text_id, text = data
            pairs.append((image_id, text))

    # æå–å”¯ä¸€å›¾ç‰‡
    unique_image_ids = sorted(set(p[0] for p in pairs))
    images = []
    with env_imgs.begin() as txn:
        for img_id in unique_image_ids:
            b64 = txn.get(str(img_id).encode("utf-8"))
            if b64:
                img_bytes = base64.b64decode(b64.decode("utf-8"))
                img = Image.open(BytesIO(img_bytes)).convert("RGB")
                images.append((img_id, preprocess(img)))

    # æ„å»º image_id â†’ æ•°ç»„ä½ç½® çš„æ˜ å°„
    imgid_to_pos = {iid: pos for pos, (iid, _) in enumerate(images)}

    # æå–å”¯ä¸€æ–‡æœ¬
    unique_texts = list(set(p[1] for p in pairs))
    text_to_idx = {text: idx for idx, text in enumerate(unique_texts)}  # O(1) æŸ¥è¡¨ä¼˜åŒ–

    # æ„å»º ground truth æ˜ å°„ï¼ˆç”¨æ•°ç»„ä½ç½®ï¼Œä¸ç”¨ image_idï¼‰
    text_to_images = {}
    image_to_texts = {}
    for img_id, text in pairs:
        text_idx = text_to_idx[text]
        img_pos = imgid_to_pos.get(img_id)
        if img_pos is None:
            continue

        if text_idx not in text_to_images:
            text_to_images[text_idx] = set()
        text_to_images[text_idx].add(img_pos)

        if img_pos not in image_to_texts:
            image_to_texts[img_pos] = set()
        image_to_texts[img_pos].add(text_idx)

    env_pairs.close()
    env_imgs.close()

    return images, unique_texts, text_to_images, image_to_texts


def load_distractors(distractor_dir: str, preprocess, start_id: int = 100000):
    """åŠ è½½å¹²æ‰°å›¾ç‰‡ï¼ˆHard Negativeï¼‰ã€‚
    
    å¹²æ‰°å›¾ç‰‡ä½¿ç”¨ä¸è®­ç»ƒæ•°æ®ä¸é‡å çš„ image_idï¼ˆä» start_id å¼€å§‹ï¼‰ï¼Œ
    ä»…æ··å…¥å›¾ç‰‡æ£€ç´¢æ± ï¼Œä¸å½±å“ ground truth æ˜ å°„ã€‚
    """
    distractor_dir = Path(distractor_dir)
    if not distractor_dir.exists():
        logger.warning(f"å¹²æ‰°å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {distractor_dir}")
        return []

    exts = {".jpg", ".jpeg", ".png", ".webp"}
    distractors = []
    for i, p in enumerate(sorted(distractor_dir.iterdir())):
        if p.suffix.lower() not in exts or not p.is_file():
            continue
        try:
            img = Image.open(p).convert("RGB")
            distractors.append((start_id + i, preprocess(img)))
        except Exception:
            continue
    return distractors


def compute_features(model, images, texts, device, batch_size=32):
    """æå–æ‰€æœ‰å›¾ç‰‡å’Œæ–‡æœ¬çš„ç‰¹å¾å‘é‡"""
    model.eval()

    # å›¾ç‰‡ç‰¹å¾
    all_img_feats = []
    for i in range(0, len(images), batch_size):
        batch = torch.stack([img for _, img in images[i:i+batch_size]]).to(device)
        with torch.no_grad():
            with torch.amp.autocast("cuda"):
                feats = model.encode_image(batch)
                feats = feats / feats.norm(dim=-1, keepdim=True)
        all_img_feats.append(feats.cpu())
    image_features = torch.cat(all_img_feats, dim=0)

    # æ–‡æœ¬ç‰¹å¾
    all_txt_feats = []
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        tokens = tokenize(batch_texts, context_length=52).to(device)
        with torch.no_grad():
            with torch.amp.autocast("cuda"):
                feats = model.encode_text(tokens)
                feats = feats / feats.norm(dim=-1, keepdim=True)
        all_txt_feats.append(feats.cpu())
    text_features = torch.cat(all_txt_feats, dim=0)

    return image_features, text_features


def metrics_at_k(sim_matrix, ground_truth, k_list=[1, 5, 10]):
    """è®¡ç®— Recall@K, Mean Recall, mAP, NDCG@K"""
    results = {}
    
    # å­˜å‚¨æŒ‡æ ‡çš„ç´¯åŠ å™¨
    recalls = {k: 0 for k in k_list}
    ndcgs = {k: 0 for k in k_list}
    map_sum = 0
    total = 0
    
    for i in range(sim_matrix.shape[0]):
        if i not in ground_truth:
            continue
            
        gt = ground_truth[i]
        num_gt = len(gt)
        if num_gt == 0: continue
            
        # å¯¹è¯¥æ ·æœ¬è·å–æ‰€æœ‰é¢„æµ‹å€¼çš„æ’åº
        pred_indices = sim_matrix[i].argsort(descending=True).tolist()
        
        # è®¡ç®— Recall@K å’Œ NDCG@K
        for k in k_list:
            topk = pred_indices[:k]
            hits = [1 if idx in gt else 0 for idx in topk]
            
            # Recall@K: æ˜¯å¦å‘½ä¸­
            if sum(hits) > 0:
                recalls[k] += 1
                
            # NDCG@K: æ ‡å‡†æŠ˜æ‰£ç´¯ç§¯æ”¶ç›Š
            dcg = sum([rel / math.log2(rank + 2) for rank, rel in enumerate(hits)])
            idcg = sum([1 / math.log2(rank + 2) for rank in range(min(num_gt, k))])
            ndcgs[k] += dcg / idcg if idcg > 0 else 0
            
        # è®¡ç®— Average Precision (AP) ç”¨äº mAP
        ap = 0
        hits_so_far = 0
        for rank, idx in enumerate(pred_indices):
            if idx in gt:
                hits_so_far += 1
                ap += hits_so_far / (rank + 1)
        map_sum += ap / num_gt
        
        total += 1
        
    for k in k_list:
        results[f"R@{k}"] = recalls[k] / max(total, 1) * 100
        results[f"NDCG@{k}"] = ndcgs[k] / max(total, 1) * 100
    results["mAP"] = map_sum / max(total, 1) * 100
    results["MR"] = sum([results[f"R@{k}"] for k in k_list]) / len(k_list)
    
    return results


def main():
    import sys
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    parser = argparse.ArgumentParser(description="Chinese-CLIP æ£€ç´¢è¯„ä¼°")
    parser.add_argument("--mode", choices=["zeroshot", "lora", "both"], default="both")
    parser.add_argument("--lora_path", type=str, default="../clip_data/experiments/lora_song/best_lora.pt")
    parser.add_argument("--data_dir", type=str, default="../clip_data/datasets/SongDynasty/lmdb/valid")
    parser.add_argument("--pretrained", type=str, default="../clip_data/pretrained_weights/")
    parser.add_argument("--rank", type=int, default=8)
    parser.add_argument("--alpha", type=float, default=16.0)
    parser.add_argument("--text_only", action="store_true", default=False, help="æ˜¯å¦ä»…å¾®è°ƒæ–‡æœ¬ç¼–ç å™¨")
    parser.add_argument("--distractor_dir", type=str, default="",
                        help="å¹²æ‰°å›¾ç‰‡ç›®å½•ï¼ˆHard Negative è¯„æµ‹ï¼‰ï¼Œå¦‚ data/distractors")
    args = parser.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"

    # åŠ è½½æ•°æ®ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰
    logger.info("ğŸ“¦ åŠ è½½æ¨¡å‹...")
    model, preprocess = load_from_name("ViT-B-16", device="cpu", download_root=args.pretrained)
    model.float()

    logger.info("ğŸ“‚ åŠ è½½è¯„ä¼°æ•°æ®...")
    images, texts, text_to_images, image_to_texts = load_eval_data(args.data_dir, preprocess)
    num_domain_images = len(images)
    logger.info(f"é¢†åŸŸå›¾ç‰‡: {num_domain_images} | æ–‡æœ¬: {len(texts)}")

    # åŠ è½½å¹²æ‰°å›¾ç‰‡ï¼ˆHard Negativeï¼‰
    if args.distractor_dir:
        distractors = load_distractors(args.distractor_dir, preprocess)
        if distractors:
            images = images + distractors  # è¿½åŠ åˆ°æœ«å°¾ï¼Œä¸å½±å“ ground truth ä½ç½®
            logger.info(f"ğŸ¯ å¹²æ‰°å›¾ç‰‡: {len(distractors)} | æ€»æ£€ç´¢æ± : {len(images)}")

    modes = [args.mode] if args.mode != "both" else ["zeroshot", "lora"]
    all_results = {}

    for mode in modes:
        logger.info(f"{'='*50}")
        logger.info(f"è¯„ä¼°æ¨¡å¼: {mode}")
        logger.info(f"{'='*50}")

        if mode == "lora":
            # é‡æ–°åŠ è½½å¹²å‡€æ¨¡å‹å†æ³¨å…¥ LoRA
            model, preprocess = load_from_name("ViT-B-16", device="cpu", download_root=args.pretrained)
            model.float()
            inject_lora(model, rank=args.rank, alpha=args.alpha, text_only=args.text_only)
            if os.path.isfile(args.lora_path):
                state_dict = torch.load(args.lora_path, map_location="cpu", weights_only=False)
                load_lora_state_dict(model, state_dict)
                logger.info(f"LoRA æƒé‡å·²åŠ è½½: {args.lora_path}")
            else:
                logger.warning(f"LoRA æƒé‡ä¸å­˜åœ¨: {args.lora_path}ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")

        model = model.to(device)
        model.eval()

        logger.info("ğŸ”§ æå–ç‰¹å¾...")
        image_features, text_features = compute_features(model, images, texts, device)

        sim_t2i = text_features @ image_features.T
        sim_i2t = image_features @ text_features.T

        t2i_metrics = metrics_at_k(sim_t2i, text_to_images)
        i2t_metrics = metrics_at_k(sim_i2t, image_to_texts)

        logger.info(f"\n  Text â†’ Image:")
        for k, v in t2i_metrics.items():
            logger.info(f"    {k}: {v:.1f}%")
        logger.info(f"\n  Image â†’ Text:")
        for k, v in i2t_metrics.items():
            logger.info(f"    {k}: {v:.1f}%")

        all_results[mode] = {
            "text_to_image": t2i_metrics,
            "image_to_text": i2t_metrics,
        }

        result = {"mode": mode, **all_results[mode],
                  "num_domain_images": num_domain_images,
                  "num_distractors": len(images) - num_domain_images,
                  "num_total_images": len(images),
                  "num_texts": len(texts)}
        with open(Path(f"eval_results_{mode}.json"), "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

    # å¦‚æœè·‘äº† both æ¨¡å¼ï¼Œæ‰“å°å¯¹æ¯”è¡¨æ ¼
    if len(all_results) == 2:
        pool_desc = f"ï¼ˆæ£€ç´¢æ± : {len(images)} å¼ ï¼Œå…¶ä¸­å¹²æ‰° {len(images) - num_domain_images}ï¼‰" if len(images) > num_domain_images else ""
        print(f"\n{'='*75}")
        print(f"  ğŸ“Š Zero-Shot vs LoRA å¯¹æ¯” {pool_desc}")
        print(f"{'='*75}")
        print(f"  {'æŒ‡æ ‡':<16} {'Zero-Shot':>10} {'LoRA':>10} {'æå‡':>10}")
        print(f"  {'-'*70}")
        zs = all_results["zeroshot"]
        lo = all_results["lora"]
        for direction, label in [("text_to_image", "Tâ†’I"), ("image_to_text", "Iâ†’T")]:
            for k in ["R@1", "R@5", "R@10", "MR", "mAP", "NDCG@5"]:
                z = zs[direction].get(k, 0)
                l = lo[direction].get(k, 0)
                delta = l - z
                sign = "+" if delta >= 0 else ""
                print(f"  {label} {k:<11} {z:>9.1f}% {l:>9.1f}% {sign}{delta:>8.1f}%")
        print(f"  {'='*75}")

    logger.info("ğŸ’¾ ç»“æœå·²ä¿å­˜")


if __name__ == "__main__":
    main()
