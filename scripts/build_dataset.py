# -*- coding: utf-8 -*-
"""
å°† annotations.json è½¬æ¢ä¸º Chinese-CLIP è®­ç»ƒæ‰€éœ€çš„æ ¼å¼ï¼š
  - {split}_imgs.tsv    (image_id \t base64)
  - {split}_texts.jsonl ({"text_id", "text", "image_ids"})
ç„¶åè°ƒç”¨ build_lmdb_dataset.py ç”Ÿæˆ LMDBã€‚

ä½¿ç”¨æ–¹æ³•:
    conda activate pytorch
    cd d:/Desktop/CV/CLIP/NanS-CLIP
    python scripts/build_dataset.py
"""

import json
import base64
import random
import argparse
from pathlib import Path
from io import BytesIO
from PIL import Image

# ============ é…ç½® ============
ANNOTATION_FILE = Path("data/annotations.json")
IMAGE_DIR = Path("data/images")
OUTPUT_DIR = Path("../clip_data/datasets/SongDynasty")  # Chinese-CLIP çº¦å®š
TRAIN_RATIO = 0.8


def image_to_base64(image_path: Path, max_size: int = 512) -> str:
    """å°†å›¾ç‰‡å‹ç¼©å¹¶è½¬ä¸º base64ï¼ˆèŠ‚çœ LMDB ç©ºé—´ï¼‰"""
    img = Image.open(image_path).convert("RGB")

    # ç­‰æ¯”ç¼©æ”¾åˆ° max_size
    w, h = img.size
    if max(w, h) > max_size:
        ratio = max_size / max(w, h)
        img = img.resize((int(w * ratio), int(h * ratio)), Image.LANCZOS)

    buf = BytesIO()
    img.save(buf, format="JPEG", quality=85)
    return base64.b64encode(buf.getvalue()).decode("utf-8")


def build_texts_for_image(ann: dict) -> list:
    """
    ä»ä¸€æ¡æ ‡æ³¨ç”Ÿæˆå¤šæ¡ text è®°å½•ã€‚
    """
    texts = []

    # 1. ç°ä»£ä¸­æ–‡æè¿°
    modern = ann.get("modern_chinese", "").strip()
    if modern:
        texts.append(modern)

    # 2. å¤æ–‡é£æ ¼æè¿°
    ancient = ann.get("ancient_style", "").strip()
    if ancient:
        texts.append(ancient)

    # 3. å…³é”®è¯ç»„åˆä¸ºä¸€å¥è¯ï¼ˆæ›´é€‚åˆ CLIP çš„çŸ­æ–‡æœ¬ç¼–ç ï¼‰
    keywords = ann.get("keywords", "").strip()
    if keywords:
        # "å—å®‹, å±±æ°´ç”», è¥¿æ¹–" â†’ "å—å®‹ å±±æ°´ç”» è¥¿æ¹–"
        kw_text = keywords.replace(",", " ").replace("ï¼Œ", " ").strip()
        texts.append(kw_text)

    # 4. æ ‡é¢˜æœ¬èº«ä¹Ÿæ˜¯ä¸€æ¡æ–‡æœ¬
    title = ann.get("title", "").strip()
    if title and title not in texts:
        texts.append(title)

    return texts


def main():
    import sys
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    parser = argparse.ArgumentParser(description="æ„å»º Chinese-CLIP æ•°æ®é›†")
    parser.add_argument("--annotation", type=str, default=str(ANNOTATION_FILE))
    parser.add_argument("--image_dir", type=str, default=str(IMAGE_DIR))
    parser.add_argument("--output_dir", type=str, default=str(OUTPUT_DIR))
    parser.add_argument("--train_ratio", type=float, default=TRAIN_RATIO)
    parser.add_argument("--seed", type=int, default=42)
    args = parser.parse_args()

    annotation_file = Path(args.annotation)
    image_dir = Path(args.image_dir)
    output_dir = Path(args.output_dir)

    # åŠ è½½æ ‡æ³¨
    with open(annotation_file, "r", encoding="utf-8") as f:
        annotations = json.load(f)
    print(f"ğŸ“‹ åŠ è½½æ ‡æ³¨: {len(annotations)} æ¡")

    # è¿‡æ»¤æ‰æ²¡æœ‰å›¾ç‰‡æ–‡ä»¶çš„è®°å½•
    valid = []
    for ann in annotations:
        img_path = image_dir / ann["filename"]
        if img_path.exists():
            valid.append(ann)
        else:
            print(f"  âš ï¸ å›¾ç‰‡ä¸å­˜åœ¨ï¼Œè·³è¿‡: {ann['filename']}")
    print(f"   æœ‰æ•ˆè®°å½•: {len(valid)} æ¡")

    if len(valid) < 5:
        print("âŒ å›¾ç‰‡æ•°é‡å¤ªå°‘ï¼ˆ<5ï¼‰ï¼Œæ— æ³•æ„å»ºæœ‰æ•ˆæ•°æ®é›†")
        return

    # æ‰“ä¹±å¹¶æŒ‰ **å›¾ç‰‡** åˆ’åˆ† train/valid é¿å…æ³„éœ²
    # åŒä¸€å¼ å›¾ç‰‡çš„æ‰€æœ‰å˜ä½“æè¿°åªä¼šåˆ†åˆ°åŒä¸€è¾¹
    unique_filenames = list(set([ann["filename"] for ann in valid]))
    random.seed(args.seed)
    random.shuffle(unique_filenames)
    
    split_img_idx = int(len(unique_filenames) * args.train_ratio)
    train_filenames = set(unique_filenames[:split_img_idx])
    
    splits = {
        "train": [ann for ann in valid if ann["filename"] in train_filenames],
        "valid": [ann for ann in valid if ann["filename"] not in train_filenames],
    }
    
    print(f"   æŒ‰å›¾ç‰‡åˆ‡åˆ†: è®­ç»ƒé›†åŒ…å« {len(train_filenames)} å¼ åŸå›¾ | éªŒè¯é›†åŒ…å« {len(unique_filenames) - len(train_filenames)} å¼ åŸå›¾")
    print(f"   æœ€ç»ˆè®°å½•æ•°: è®­ç»ƒé›† {len(splits['train'])} æ¡ | éªŒè¯é›† {len(splits['valid'])} æ¡")

    # ç”Ÿæˆæ•°æ®æ–‡ä»¶
    output_dir.mkdir(parents=True, exist_ok=True)

    for split_name, split_data in splits.items():
        tsv_path = output_dir / f"{split_name}_imgs.tsv"
        jsonl_path = output_dir / f"{split_name}_texts.jsonl"

        text_id_counter = 0

        # ç”±äºå¯èƒ½æœ‰å¤šæ¡æ ‡æ³¨å¯¹åº”åŒä¸€å¼ å›¾ï¼ˆè™½ç„¶ä¸Šé¢çš„å¤„ç†ä¿è¯äº†åŒä¸€å›¾éƒ½åœ¨åŒä¸€ä¸ª splitï¼‰ï¼Œ
        # æˆ‘ä»¬éœ€è¦åœ¨ç”Ÿæˆ TSV æ—¶æŒ‰å»é‡çš„å›¾ç‰‡å†™å…¥ä»¥é˜²æŠ¥é”™ã€‚
        # å»ºç«‹å»é‡æ˜ å°„
        split_unique_ann = []
        seen = set()
        for ann in split_data:
            if ann["filename"] not in seen:
                split_unique_ann.append(ann)
                seen.add(ann["filename"])

        with open(tsv_path, "w", encoding="utf-8") as f_tsv, \
             open(jsonl_path, "w", encoding="utf-8") as f_jsonl:

            # éå†å»é‡åçš„å›¾ç‰‡
            for image_id, ann_master in enumerate(split_unique_ann):
                img_path = image_dir / ann_master["filename"]

                # å†™ TSVï¼ˆå›¾ç‰‡ base64ï¼‰
                try:
                    b64 = image_to_base64(img_path)
                    f_tsv.write(f"{image_id}\t{b64}\n")
                except Exception as e:
                    print(f"  âš ï¸ å›¾ç‰‡å¤„ç†å¤±è´¥: {ann_master['filename']}: {e}")
                    continue

                # æ”¶é›†æ‰€æœ‰æ­¤å›¾ç‰‡çš„æ ‡æ³¨
                ann_list = [a for a in split_data if a["filename"] == ann_master["filename"]]
                
                # å†™ JSONL
                for ann in ann_list:
                    texts = build_texts_for_image(ann)
                    for text in texts:
                        entry = {
                            "text_id": text_id_counter,
                            "text": text,
                            "image_ids": [image_id]
                        }
                        f_jsonl.write(json.dumps(entry, ensure_ascii=False) + "\n")
                        text_id_counter += 1

        print(f"   âœ… {split_name}: {len(split_data)} å›¾ | {text_id_counter} æ–‡æœ¬å¯¹")
        print(f"      {tsv_path}")
        print(f"      {jsonl_path}")

    print(f"\nğŸ“ æ•°æ®æ–‡ä»¶è¾“å‡º: {output_dir.resolve()}")
    print(f"\nä¸‹ä¸€æ­¥ï¼šè¿è¡Œ LMDB è½¬æ¢:")
    print(f"  python cn_clip/preprocess/build_lmdb_dataset.py \\")
    print(f"    --data_dir {output_dir} --splits train,valid")


if __name__ == "__main__":
    main()
