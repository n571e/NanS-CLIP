# -*- coding: utf-8 -*-
"""
VLM æ ‡æ³¨è´¨é‡è¿‡æ»¤å™¨ã€‚

ç”¨ Chinese-CLIP åå‘è®¡ç®—æ¯æ¡æ ‡æ³¨æ–‡æœ¬ä¸å¯¹åº”å›¾ç‰‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦,
å‰”é™¤ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼çš„ä½è´¨é‡æ ‡æ³¨ï¼Œç¡®ä¿è®­ç»ƒæ•°æ®è´¨é‡ã€‚

ç”¨æ³•:
    python scripts/filter_annotations.py
    python scripts/filter_annotations.py --threshold 0.15 --dry-run
"""

import argparse
import json
import sys
from pathlib import Path

import torch
from PIL import Image

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import cn_clip.clip as clip
from cn_clip.clip import load_from_name

ANNOTATION_FILE = PROJECT_ROOT / "data" / "annotations.json"
IMAGE_DIR = PROJECT_ROOT / "data" / "images"


def compute_similarity(model, preprocess, tokenizer_fn, image_path, text, device):
    """è®¡ç®—å•å¼ å›¾ç‰‡ä¸ä¸€æ¡æ–‡æœ¬ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
    # å›¾åƒç¼–ç 
    image = preprocess(Image.open(image_path).convert("RGB")).unsqueeze(0).to(device)
    # æ–‡æœ¬ç¼–ç 
    text_tokens = tokenizer_fn([text]).to(device)

    with torch.no_grad():
        image_features = model.encode_image(image)
        text_features = model.encode_text(text_tokens)

        # L2 å½’ä¸€åŒ–
        image_features = image_features / image_features.norm(dim=-1, keepdim=True)
        text_features = text_features / text_features.norm(dim=-1, keepdim=True)

        # ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = (image_features @ text_features.T).item()

    return similarity


def main():
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', line_buffering=True)

    parser = argparse.ArgumentParser(description="ç”¨ CLIP è¿‡æ»¤ä½è´¨é‡ VLM æ ‡æ³¨")
    parser.add_argument("--annotation", type=str, default=str(ANNOTATION_FILE))
    parser.add_argument("--threshold", type=float, default=0.15,
                        help="ä½™å¼¦ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„æ ‡æ³¨å°†è¢«æ ‡è®°ä¸ºä½è´¨é‡ (é»˜è®¤ 0.15)")
    parser.add_argument("--pretrained", type=str, default="../clip_data/pretrained_weights/")
    parser.add_argument("--dry-run", action="store_true",
                        help="åªæŠ¥å‘Šä¸å®é™…åˆ é™¤ï¼Œç”¨äºæŸ¥çœ‹é˜ˆå€¼æ•ˆæœ")
    args = parser.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ”§ è®¾å¤‡: {device}")

    # åŠ è½½ CLIP æ¨¡å‹ï¼ˆZero-Shotï¼Œä¸åŠ è½½ LoRAï¼Œç”¨åŸå§‹æ¨¡å‹è¯„ä¼°ï¼‰
    print("ğŸ“¦ åŠ è½½ Chinese-CLIP ViT-B-16 (Zero-Shot) ç”¨äºè´¨é‡è¯„ä¼°...")
    model, preprocess = load_from_name("ViT-B-16", device=device,
                                        download_root=args.pretrained)
    model.eval()

    # åŠ è½½æ ‡æ³¨
    annotation_file = Path(args.annotation)
    with open(annotation_file, "r", encoding="utf-8") as f:
        annotations = json.load(f)
    print(f"ğŸ“‹ åŠ è½½ {len(annotations)} æ¡æ ‡æ³¨\n")

    kept = []
    removed = []

    for i, ann in enumerate(annotations):
        filename = ann.get("filename", "")
        img_path = IMAGE_DIR / filename

        if not img_path.exists():
            print(f"[{i+1}/{len(annotations)}] âš ï¸ å›¾ç‰‡ä¸å­˜åœ¨: {filename}ï¼Œè·³è¿‡")
            removed.append(ann)
            continue

        # å¯¹ modern_chinese è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆå®ƒæ˜¯æœ€æ ¸å¿ƒçš„æè¿°æ–‡æœ¬ï¼‰
        text = ann.get("modern_chinese", "")
        if not text:
            print(f"[{i+1}/{len(annotations)}] âš ï¸ æ— æ–‡æœ¬: {filename}ï¼Œç§»é™¤")
            removed.append(ann)
            continue

        sim = compute_similarity(model, preprocess, clip.tokenize, img_path, text, device)

        if sim < args.threshold:
            print(f"[{i+1}/{len(annotations)}] âŒ sim={sim:.4f} < {args.threshold} | {filename}")
            print(f"    æ–‡æœ¬: {text[:60]}...")
            removed.append(ann)
        else:
            if (i + 1) % 20 == 0 or i == 0:
                print(f"[{i+1}/{len(annotations)}] âœ… sim={sim:.4f} | {filename}")
            kept.append(ann)

    # æ±‡æ€»
    print(f"\n{'='*50}")
    print(f"  æ€»è®¡: {len(annotations)} æ¡")
    print(f"  ä¿ç•™: {len(kept)} æ¡")
    print(f"  å‰”é™¤: {len(removed)} æ¡ (ç›¸ä¼¼åº¦ < {args.threshold})")
    print(f"{'='*50}")

    if removed:
        print(f"\nè¢«å‰”é™¤çš„æ ‡æ³¨:")
        for r in removed:
            print(f"  - {r.get('filename', '?')}: {r.get('modern_chinese', '')[:50]}")

    if not args.dry_run and removed:
        # å¤‡ä»½åŸæ–‡ä»¶
        backup = annotation_file.with_suffix(".json.bak")
        annotation_file.rename(backup)
        print(f"\nğŸ’¾ åŸæ–‡ä»¶å·²å¤‡ä»½: {backup}")

        # å†™å…¥è¿‡æ»¤åçš„æ–‡ä»¶
        with open(annotation_file, "w", encoding="utf-8") as f:
            json.dump(kept, f, ensure_ascii=False, indent=2)
        print(f"âœ… è¿‡æ»¤åæ ‡æ³¨å·²ä¿å­˜: {annotation_file} ({len(kept)} æ¡)")
    elif args.dry_run:
        print(f"\nğŸ” [Dry Run] æœªå®é™…ä¿®æ”¹æ–‡ä»¶ã€‚å»æ‰ --dry-run ä»¥æ‰§è¡Œè¿‡æ»¤ã€‚")


if __name__ == "__main__":
    main()
