# -*- coding: utf-8 -*-
"""
é€šç”¨å¹²æ‰°å›¾ç‰‡çˆ¬è™« â€”â€” ç”¨äº Hard Negative è¯„æµ‹ã€‚

çˆ¬å–ä¸å—å®‹æ–‡åŒ–æ— å…³çš„é€šç”¨å›¾ç‰‡ï¼Œå­˜æ”¾åœ¨ç‹¬ç«‹ç›®å½• data/distractors/ï¼Œ
ä¸ä¼šæ±¡æŸ“å—å®‹è®­ç»ƒæ•°æ®é›†ã€‚

ç”¨æ³•:
    python scripts/scrape_distractors.py
"""

import json
import os
import re
import time
import random
from pathlib import Path
from hashlib import md5

try:
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
except ImportError:
    print("âŒ ç¼ºå°‘ requests åº“: pip install requests")
    import sys; sys.exit(1)

# ========== é…ç½® ==========
# ç‹¬ç«‹ç›®å½•ï¼Œä¸å—å®‹æ•°æ®é›†å®Œå…¨åˆ†å¼€
IMAGE_DIR = Path("data/distractors")
METADATA_FILE = Path("data/distractors_metadata.jsonl")

MAX_PER_QUERY = 10
DOWNLOAD_DELAY = 0.8
MIN_IMAGE_SIZE = 20_000  # 20KB

# é€šç”¨å…³é”®è¯ï¼ˆå’Œå—å®‹æ–‡åŒ–å®Œå…¨æ— å…³çš„é¢†åŸŸï¼‰
DISTRACTOR_QUERIES = [
    # --- Hard Negatives (é¢†åŸŸå†…éš¾è´Ÿæ ·æœ¬) ---
    "æ˜ä»£ æ°´å¢¨ç”»",
    "æ¸…ä»£ å±±æ°´é•¿å·",
    "æ±‰ä»£ ç”»åƒçŸ³ ç”»",
    "å”ä»£ å£ç”» æ•¦ç…Œ",
    "æ—¥æœ¬ æµ®ä¸–ç»˜",
    "ç°ä»£ å›½ç”» å±±æ°´",
    "ç°ä»£ ä»¿å¤ é™¶ç“·",
    "æ¸…ä»£ ç²‰å½© ç“·å™¨",
    "å•†ä»£ é’é“œå™¨",
    "è¥¿æ–¹ å¤å…¸æ²¹ç”» é£æ™¯",
    "ç°ä»£ ä¹¦æ³• å±•è§ˆ",
    "æ¬§æ´² ä¸­ä¸–çºª æ‰‹æŠ„æœ¬",
    "å¤å¸Œè…Š é›•å¡‘",
    # --- Easy Negatives (é€šç”¨å¹²æ‰°é¡¹) ---
    "ç°ä»£åŸå¸‚å»ºç­‘ é«˜æ¸…æ‘„å½±",
    "åŒ—äº¬å¤©å®‰é—¨å¹¿åœº ç…§ç‰‡",
    "ä¸Šæµ·é™†å®¶å˜´ å¤œæ™¯",
    "é«˜é“ ä¸­å›½ ç…§ç‰‡",
    "æ‰‹æœº æ•°ç äº§å“ æ‘„å½±",
    "çŒ«å’ª å¯çˆ± é«˜æ¸…",
    "å¤§ç†ŠçŒ« å›½å®",
    "é‡‘æ¯› çŠ¬ å® ç‰©",
    "ä¸­å›½ç¾é£Ÿ é«˜æ¸…",
    "ç«é”… å·èœ æ‘„å½±",
    "è¥¿é¤ ç‰›æ’ ç¾é£Ÿ",
    "é›ªå±± æ—¥å‡º é£æ™¯",
    "å¤§æµ· æ²™æ»© å¤å¤©",
    "ç¯®çƒ NBA æ¯”èµ›",
    "äººå·¥æ™ºèƒ½ æœºå™¨äºº",
    "èŠ¯ç‰‡ åŠå¯¼ä½“ å¾®è·",
]

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
]


def sanitize_fn(name):
    name = re.sub(r'[<>:"/\\|?*\x00-\x1f]', '_', name)
    name = re.sub(r'\s+', '_', name)
    return name[:60]


def is_image_data(data):
    return (data[:3] == b'\xff\xd8\xff' or
            data[:8] == b'\x89PNG\r\n\x1a\n' or
            data[:4] == b'RIFF' or
            data[:4] == b'GIF8')


def main():
    import sys, io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', line_buffering=True)

    IMAGE_DIR.mkdir(parents=True, exist_ok=True)

    session = requests.Session()
    retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
    session.mount("http://", HTTPAdapter(max_retries=retries))
    session.mount("https://", HTTPAdapter(max_retries=retries))
    import urllib3; urllib3.disable_warnings()

    # æ–­ç‚¹ç»­çˆ¬
    existing_urls = set()
    existing_count = 0
    if METADATA_FILE.exists():
        with open(METADATA_FILE, "r", encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    try:
                        entry = json.loads(line.strip())
                        existing_urls.add(entry.get("original_url", ""))
                        existing_count += 1
                    except: pass

    print(f"ğŸ¯ å¹²æ‰°å›¾ç‰‡çˆ¬è™«ï¼ˆHard Negative è¯„æµ‹ç”¨ï¼‰")
    print(f"   å­˜æ”¾ç›®å½•: {IMAGE_DIR.resolve()}")
    print(f"   å·²æœ‰: {existing_count} å¼ \n")

    idx = existing_count
    new_entries = []

    for qi, q in enumerate(DISTRACTOR_QUERIES):
        print(f"[{qi+1}/{len(DISTRACTOR_QUERIES)}] {q}")

        params = {
            "tn": "resultjson_com", "ipn": "rj", "ct": "201326592",
            "fp": "result", "word": q, "queryWord": q, "cl": "2",
            "lm": "-1", "ie": "utf-8", "oe": "utf-8", "st": "-1",
            "ic": "0", "istype": "2", "pn": "0", "rn": "20",
        }

        try:
            url = "https://image.baidu.com/search/acjson?" + "&".join(f"{k}={v}" for k, v in params.items())
            hdrs = {
                "User-Agent": random.choice(USER_AGENTS),
                "Referer": "https://image.baidu.com/",
                "Accept": "application/json",
            }
            resp = session.get(url, headers=hdrs, verify=False, timeout=15)
            if resp.status_code != 200:
                print(f"  è·³è¿‡ (HTTP {resp.status_code})"); continue
            text = resp.text.replace("\\'", "'")
            data = json.loads(text)
        except Exception as e:
            print(f"  APIå¤±è´¥: {str(e)[:50]}"); continue

        items = data.get("data", [])
        cnt = 0
        for item in items:
            if not isinstance(item, dict) or cnt >= MAX_PER_QUERY:
                continue
            img_url = item.get("hoverURL") or item.get("middleURL") or item.get("thumbURL", "")
            if not img_url or img_url in existing_urls:
                continue

            h = md5(img_url.encode()).hexdigest()[:8]
            fn = f"dist_{idx:04d}_{sanitize_fn(q[:15])}_{h}.jpg"

            try:
                r = session.get(img_url, headers={"Referer": "https://image.baidu.com/",
                                                   "User-Agent": random.choice(USER_AGENTS)},
                                verify=False, timeout=15)
                content = r.content
                if len(content) < MIN_IMAGE_SIZE or not is_image_data(content):
                    continue
                with open(IMAGE_DIR / fn, "wb") as f:
                    f.write(content)

                new_entries.append({
                    "filename": fn,
                    "query": q,
                    "original_url": img_url,
                    "source": "distractor",
                })
                existing_urls.add(img_url)
                idx += 1
                cnt += 1
            except:
                continue
            time.sleep(DOWNLOAD_DELAY)

        print(f"  +{cnt}")

    # ä¿å­˜
    if new_entries:
        with open(METADATA_FILE, "a", encoding="utf-8") as f:
            for entry in new_entries:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")

    total = existing_count + len(new_entries)
    print(f"\n{'='*50}")
    print(f"ğŸ‰ å®Œæˆ! æ–°å¢ {len(new_entries)} å¼ ï¼Œç´¯è®¡ {total} å¼ å¹²æ‰°å›¾ç‰‡")
    print(f"   ğŸ“‚ {IMAGE_DIR.resolve()}")
    print(f"   ğŸ“ {METADATA_FILE.resolve()}")
    print(f"{'='*50}")


if __name__ == "__main__":
    main()
