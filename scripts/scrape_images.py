# -*- coding: utf-8 -*-
"""
å—å®‹æ–‡åŒ–å›¾ç‰‡å¢å¼ºç‰ˆå¤šæ¸ é“çˆ¬è™« (ä½¿ç”¨ requests)

ä¾èµ–:
    pip install requests
    conda install requests
"""

import os
import re
import json
import time
import random
from pathlib import Path
from hashlib import md5
import argparse

try:
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
except ImportError:
    print("âŒ ç¼ºå°‘ requests åº“ï¼Œè¯·å…ˆè¿è¡Œ: pip install requests")
    import sys
    sys.exit(1)

# ============ é…ç½® ============
IMAGE_DIR = Path("data/images")
METADATA_FILE = Path("data/image_metadata.jsonl")

# éšæœº User-Agent åº“ï¼Œé¿å…è¢«è¯†åˆ«ä¸ºå›ºå®šçˆ¬è™«
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
]

MAX_PER_QUERY = 60         # æ¯ä¸ªå…³é”®è¯ä¸‹è½½æ•°é‡ (ç›®æ ‡: 3000+ å¼ )
DOWNLOAD_DELAY = 0.5       # ä¸‹è½½é—´éš”
MIN_IMAGE_SIZE = 20_000    # 20KB


def get_session():
    """åˆ›å»ºä¸€ä¸ªå¸¦æœ‰è‡ªåŠ¨é‡è¯•æœºåˆ¶çš„ Requests Session"""
    session = requests.Session()
    # è®¾ç½®é‡è¯•ç­–ç•¥ï¼šé‡åˆ° 429(é™æµ), 500, 502, 503, 504 ä¼šè‡ªåŠ¨é€€é¿å¹¶é‡è¯•
    retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
    adapter = HTTPAdapter(max_retries=retries)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    # ç¦ç”¨ SSL éªŒè¯è­¦å‘Š
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    return session


def make_request(session, url, headers=None, **kwargs):
    """é€šç”¨çš„ GET è¯·æ±‚ï¼Œå¸¦æœ‰éšæœº UA"""
    hdrs = {
        "User-Agent": random.choice(USER_AGENTS),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    }
    if headers:
        hdrs.update(headers)
    
    # ç¦ç”¨ SSL éªŒè¯ä»¥åº”å¯¹éƒ¨åˆ†å›½å†…å›¾åºŠè¯ä¹¦é—®é¢˜
    kwargs.setdefault("verify", False)
    kwargs.setdefault("timeout", 15)
    
    return session.get(url, headers=hdrs, **kwargs)


def sanitize_fn(name):
    """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
    name = re.sub(r'[<>:"/\\|?*\x00-\x1f]', '_', name)
    name = re.sub(r'\s+', '_', name)
    return name[:80]


def is_image_data(data):
    """é€šè¿‡æ–‡ä»¶å¤´ç­¾ååˆ¤æ–­æ˜¯å¦çœŸçš„æ˜¯å›¾ç‰‡"""
    return (data[:3] == b'\xff\xd8\xff' or          # JPEG
            data[:8] == b'\x89PNG\r\n\x1a\n' or     # PNG
            data[:4] == b'RIFF' or                   # WEBP
            data[:4] == b'GIF8')                     # GIF


def download(session, url, save_path, referer=None):
    """ä¸‹è½½å›¾ç‰‡å¹¶ä¿å­˜"""
    try:
        hdrs = {}
        if referer:
            hdrs["Referer"] = referer
        resp = make_request(session, url, headers=hdrs, stream=True)
        resp.raise_for_status()
        
        data = resp.content
        if len(data) < MIN_IMAGE_SIZE:
            print(f"      - è·³è¿‡ (å¤ªå°: {len(data)//1024}KB)")
            return False
        if not is_image_data(data):
            print("      - è·³è¿‡ (éæœ‰æ•ˆå›¾ç‰‡æ ¼å¼)")
            return False
            
        with open(save_path, "wb") as f:
            f.write(data)
        return True
    except Exception as e:
        # åªæ‰“å°ç®€çŸ­é”™è¯¯
        print(f"      x å¤±è´¥: {str(e)[:50]}")
        return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  æ•°æ®æº 1: ç™¾åº¦å›¾ç‰‡ï¼ˆæœ€ç¨³ã€ä¸­æ–‡æœç´¢ä½“éªŒæœ€å¥½ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BAIDU_QUERIES = [
    # --- å—å®‹ç»˜ç”» (å±±æ°´æ ¸å¿ƒ) ---
    "å—å®‹ å±±æ°´ç”» åšç‰©é¦†", "é©¬è¿œ è¸æ­Œå›¾ é«˜æ¸…", "é©¬è¿œ å¯’æ±Ÿç‹¬é’“å›¾", "é©¬è¿œ ç§‹æ±Ÿæ¸”éšå›¾", "é©¬è¿œ æ¢…çŸ³æºªå‡«å›¾",
    "å¤åœ­ æºªå±±æ¸…è¿œå›¾", "å¤åœ­ æ¢§ç«¹å¹½å±…å›¾", "å¤åœ­ ä¸´æµæŠšç´å›¾", "å¤åœ­ å±±æ°´åäºŒæ™¯",
    "æå” ä¸‡å£‘æ¾é£å›¾", "æå” é‡‡è–‡å›¾", "æå” æ±Ÿå±±å°æ™¯å›¾",
    "åˆ˜æ¾å¹´ å››æ™¯å±±æ°´å›¾", "åˆ˜æ¾å¹´ ç½—æ±‰å›¾", "åˆ˜æ¾å¹´ é†‰åœ£å›¾",
    "å—å®‹ é™¢ä½“ç”» é£æ ¼", "å—å®‹ è¾¹è§’ä¹‹æ™¯ æ„å›¾", "å¤§æ–§åŠˆçš´ æŠ€æ³•", "æ°´å¢¨ æ™•æŸ“ æŠ€æ³• å®‹ç”»",
    "è¥¿æ¹–åæ™¯ å¤ç”» ç»¢æœ¬", "æ–­æ¡¥æ®‹é›ª å¤ç”»", "é›·å³°å¤•ç…§ å¤ç”»", "å—å±æ™šé’Ÿ å¤ç”»",

    # --- å—å®‹ç»˜ç”» (å…¶ä»–åå®¶) ---
    "æ¢æ¥· æ³¼å¢¨ä»™äººå›¾", "æ¢æ¥· å¤ªç™½è¡ŒåŸå›¾", "æ¢æ¥· å…­ç¥–æ–«ç«¹å›¾",
    "ç‰§æºª æ½‡æ¹˜å…«æ™¯å›¾", "ç‰§æºª æ¸”æ‘å¤•ç…§å›¾", "ç‰§æºª æŸ¿å›¾", "ç‰§æºª çŒ¿å›¾",
    "èµµå­Ÿåš å¢¨å…°å›¾", "èµµå­Ÿåš æ°´ä»™å›¾", "é™ˆå®¹ ä¹é¾™å›¾", "é™ˆå®¹ äº‘é¾™å›¾",
    "æ—æ¤¿ æœç†Ÿç¦½æ¥å›¾", "æ—æ¤¿ æ¢…ç«¹å¯’ç¦½å›¾", "å´ç‚³ å‡ºæ°´èŠ™è“‰å›¾", "æè¿ª æ«é¹°é›‰é¸¡å›¾",
    "å—å®‹ ä½šå å®«å»·ç”»", "å—å®‹ è´§éƒå›¾ æåµ©", "å—å®‹ éª·é«…å¹»æˆå›¾",

    # --- é™¶ç“·ä¸å™¨ç‰© (æ·±åº¦ç»†åˆ†) ---
    "å—å®‹å®˜çª‘ ç“·å™¨ å…¸å‹å™¨", "å—å®‹ å®˜çª‘ å†°è£‚çº¹ ç»†èŠ‚", "å—å®‹ å®˜çª‘ ç²‰é’é‡‰ å¼¦çº¹ç“¶", 
    "å—å®‹ å®˜çª‘ é¬²å¼ç‚‰", "å—å®‹ å®˜çª‘ è´¯è€³ç“¶", "æ­å·è€è™æ´çª‘å€ å‡ºåœŸ", "ä¿®å†…å¸å®˜çª‘ ç“·ç‰‡",
    "é¾™æ³‰é’ç“· å—å®‹ å…¸å‹å™¨", "é¾™æ³‰çª‘ æ¢…å­é’ é¬²å¼ç‚‰", "é¾™æ³‰çª‘ ç²‰é’ å‡¤è€³ç“¶", "é¾™æ³‰çª‘ è´´èŠ± ç“·å™¨",
    "å—å®‹ å»ºç› å…”æ¯«", "å—å®‹ å»ºç› é¹§é¸ªæ–‘", "å»ºçª‘ æ›œå˜å¤©ç›® ç»†èŠ‚", "å‰å·çª‘ æœ¨å¶çº¹ ç›",
    "å‰å·çª‘ å‰ªçº¸è´´èŠ± ç›", "å‰å·çª‘ ç³ç‘é‡‰", "å—å®‹ å“¥çª‘ ä¼ ä¸– ç“·å™¨", "å“¥çª‘ é‡‘ä¸é“çº¿ ç»†èŠ‚",
    "å®‹ä»£ å®šçª‘ åˆ»èŠ± ç™½ç“·", "å®‹ä»£ æ™¯å¾·é•‡ é’ç™½ç“· å—å®‹", "å—å®‹ æ¹–ç”°çª‘ æ•",
    
    # --- é‡‘é“¶æ¼†æœ¨ä¸çººç»‡ ---
    "å—å®‹ é‡‘é“¶å™¨ å‘å‡ºåœŸ", "å—å®‹ éé‡‘ ç¼ æ ç°ª", "å—å®‹ é“¶é“¤ é“­æ–‡", "é‡‘å å…°æºª å—å®‹ å¢“å‡ºåœŸ",
    "å—å®‹ èºé’¿ æ¼†å™¨", "å—å®‹ å‰”çº¢ æ¼†å¥", "å—å®‹ å‰”çŠ€ æ¼†ç›˜", "å—å®‹ æ¼†å™¨ åšç‰©é¦†",
    "å—å®‹ ç¼‚ä¸ ç»‡ç‰©", "å®‹ä»£ åˆºç»£ é’ˆæ³• ç»†èŠ‚", "å—å®‹ é“œé•œ æ¹–å·é“­æ–‡", "å—å®‹ ç‰å™¨ é¥°å“",

    # --- å»ºç­‘ä¸è€ƒå¤é—å€ ---
    "å—å®‹ å¾·å¯¿å®« é‡å»º å»ºç­‘ç»†èŠ‚", "æ­å· å—å®‹çš‡åŸ é—å€ å…¬å›­", "ä¸´å®‰åŸ é—å€ å¤¯åœŸ", 
    "å—å®‹ å¤ªåº™ é—å€", "å…­å’Œå¡” ç –çŸ³ç»“æ„", "æ­å· é£æ¥å³° å®‹ä»£é€ åƒ", "çµéšå¯º è‹å ¤ æ˜¥æ™“",
    "å—å®‹ å…«å¦ç”° èˆªæ‹", "å°æ²³ç›´è¡— å®‹æ–‡åŒ–", "æ¡¥è¥¿ç›´è¡— å¤å»ºç­‘",

    # --- å…¸ç±ä¹¦æ³•ä¸ç”Ÿæ´» ---
    "å®‹ä»£ è´è¶è£… åˆ»æœ¬", "å—å®‹ ä¸´å®‰ åˆŠæœ¬", "å—å®‹ å®‹é«˜å®— ä¹¦æ³•", "å²³é£ æ‰‹è¿¹ å¢¨å®",
    "é™†æ¸¸ ä¹¦æ³• æ‹“ç‰‡", "èŒƒæˆå¤§ å¢¨è¿¹ è¯—å¸–", "å¼ å³ä¹‹ ä¹¦æ³• æ¥·ä¹¦", "å—å®‹ æŠ„æœ¬ æ–‡çŒ®",
    "å®‹ä»£ ç‚¹èŒ¶åœºæ™¯ ç”»", "å®‹ä»£ æ–—èŒ¶å›¾", "å—å®‹ ç„šé¦™ æŒ‚ç”» ç‚¹èŒ¶ æ’èŠ±", "æ¸…æ²³åŠ å®‹ä»£é£æƒ…",
    "å—å®‹ é“œé’± çš‡å®‹é€šå®", "å—å®‹ é“é’± é—è¿¹", "å—å®‹ åœ°å›¾ èˆ†å›¾ æ­å·"
]
def scrape_baidu(session, existing_urls, start_idx):
    print("\n" + "=" * 55)
    print("  â­ [æ•°æ®æº 1] ç™¾åº¦å›¾ç‰‡æé€Ÿç‰ˆ")
    print("=" * 55)
    entries, idx = [], start_idx

    for qi, q in enumerate(BAIDU_QUERIES):
        print(f"\n  [{qi+1}/{len(BAIDU_QUERIES)}] {q}")
        cnt = 0
        for page in range(3): # å°è¯•æŠ“å– 3 é¡µ
            if cnt >= MAX_PER_QUERY: break
            pn = page * 30
            
            params = {
                "tn": "resultjson_com", "ipn": "rj", "ct": "201326592",
                "fp": "result", "word": q, "queryWord": q, "cl": "2",
                "lm": "-1", "ie": "utf-8", "oe": "utf-8", "st": "-1",
                "ic": "0", "istype": "2", "pn": str(pn), "rn": "30",
            }
            
            try:
                url = "https://image.baidu.com/search/acjson?" + "&".join(f"{k}={v}" for k, v in params.items())
                hdrs = {"Referer": "https://image.baidu.com/", "Accept": "application/json"}
                resp = make_request(session, url, headers=hdrs)
                
                if resp.status_code != 200:
                    print(f"    æ¥å£è¯·æ±‚å¤±è´¥ ({resp.status_code})"); continue
                data = json.loads(resp.text.replace("\\'", "'"))
            except Exception as e:
                print(f"    API å¤±è´¥: {str(e)[:60]}"); continue

            items = data.get("data", [])
            for item in items:
                if not isinstance(item, dict): continue
                if cnt >= MAX_PER_QUERY: break
                
                img_url = item.get("hoverURL") or item.get("middleURL") or item.get("thumbURL") or item.get("objURL", "")
                if not img_url or img_url in existing_urls:
                    continue
                    
                from_title = item.get("fromPageTitleEnc", "") or item.get("fromPageTitle", "")
                from_title = re.sub(r'<[^>]+>', '', from_title)
                
                h = md5(img_url.encode()).hexdigest()[:8]
                ext = "jpg" if ".jpg" in img_url.lower() else "png"
                fn = f"baidu_{idx:03d}_{sanitize_fn(q[:20])}_{h}.{ext}"
                
                if download(session, img_url, IMAGE_DIR / fn, referer="https://image.baidu.com/"):
                    print(f"    [{cnt+1}/{MAX_PER_QUERY}] {fn}")
                    entries.append({
                        "filename": fn, "title": from_title or q,
                        "description": f"ç™¾åº¦å›¾ç‰‡: {q}", "categories": ["ç™¾åº¦å›¾ç‰‡"],
                        "original_url": img_url, "source": "Baidu Images",
                    })
                    existing_urls.add(img_url)
                    idx += 1
                    cnt += 1
                time.sleep(DOWNLOAD_DELAY * random.uniform(0.5, 1.5))
            
        print(f"    å…±è®¡æ–°å¢: {cnt}")
    return entries, idx


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  æ•°æ®æº 2: Wikimedia Commons
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
WIKI_QUERIES = [
    "Southern Song dynasty painting",
    "West Lake Hangzhou",
    "Song dynasty Longquan celadon",
    "Ma Yuan painting",
    "Xia Gui painting",
    "Southern Song Ceramics",
    "Southern Song Calligraphy"
]

def scrape_wikimedia(session, existing_urls, start_idx):
    print("\n" + "=" * 55)
    print("  [æ•°æ®æº 2] Wikimedia Commons")
    print("=" * 55)
    entries, idx = [], start_idx

    for qi, q in enumerate(WIKI_QUERIES):
        print(f"\n  [{qi+1}/{len(WIKI_QUERIES)}] {q}")

        params = {
            "action": "query", "list": "search", "srsearch": q, 
            "srnamespace": "6", "srlimit": "10", "format": "json"
        }
        try:
            url = "https://commons.wikimedia.org/w/api.php?" + "&".join(f"{k}={v}" for k, v in params.items())
            data = make_request(session, url).json()
            results = data.get("query", {}).get("search", [])
            if not results:
                print("    æ— ç»“æœ"); continue
                
            titles = [r["title"] for r in results]
            
            url2 = "https://commons.wikimedia.org/w/api.php"
            params2 = {
                "action": "query", "titles": "|".join(titles),
                "prop": "imageinfo", "iiprop": "url|mime",
                "iiurlwidth": "800", "format": "json"
            }
            info = make_request(session, f"{url2}?{'&'.join(f'{k}={v}' for k, v in params2.items())}").json()
            pages = info.get("query", {}).get("pages", {})
            
            cnt = 0
            for pid, pg in pages.items():
                if pid == "-1" or cnt >= MAX_PER_QUERY: continue
                ii = pg.get("imageinfo", [{}])[0]
                mime = ii.get("mime", "")
                if mime not in ("image/jpeg", "image/png"): continue
                
                # è¯·æ±‚ç¼©ç•¥å›¾ä»¥é¿å… 429
                url = ii.get("thumburl", "") or ii.get("url", "")
                if not url or url in existing_urls: continue
                
                title = pg.get("title", "").replace("File:", "")
                ext = "jpg" if "jpeg" in mime else "png"
                fn = f"wiki_{idx:03d}_{sanitize_fn(title[:30])}.{ext}"
                
                print(f"    -> {fn}")
                if download(session, url, IMAGE_DIR / fn, referer="https://commons.wikimedia.org/"):
                    entries.append({"filename": fn, "title": title, "description": "",
                                    "categories": ["Wiki"], "original_url": url,
                                    "source": "Wikimedia Commons"})
                    existing_urls.add(url); idx += 1; cnt += 1
                time.sleep(DOWNLOAD_DELAY * 2) # Wiki å®¹æ˜“ 429ï¼Œæ…¢ç‚¹çˆ¬
            print(f"    +{cnt}")
        except Exception as e:
            print(f"    è·å–å¤±è´¥: {str(e)[:50]}")
            
    return entries, idx


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  æ•°æ®æº 3: å¤§éƒ½ä¼šè‰ºæœ¯åšç‰©é¦† Open Access APIï¼ˆå…è´¹ï¼‰
#  https://metmuseum.github.io/
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
MET_QUERIES = [
    "Song dynasty painting",
    "Song dynasty ceramics",
    "Song dynasty calligraphy",
    "Chinese landscape painting Song",
    "Song dynasty jade",
    "Song dynasty bronze",
    "Hangzhou",
]

def scrape_met_museum(session, existing_urls, start_idx):
    print("\n" + "=" * 55)
    print("  [æ•°æ®æº 3] å¤§éƒ½ä¼šè‰ºæœ¯åšç‰©é¦† Open Access")
    print("=" * 55)
    entries, idx = [], start_idx
    base = "https://collectionapi.metmuseum.org/public/collection/v1"

    for qi, q in enumerate(MET_QUERIES):
        print(f"\n  [{qi+1}/{len(MET_QUERIES)}] {q}")
        try:
            resp = make_request(session, f"{base}/search?q={q}&hasImages=true")
            data = resp.json()
            obj_ids = data.get("objectIDs", [])
            if not obj_ids:
                print("    æ— ç»“æœ"); continue

            cnt = 0
            for oid in obj_ids[:15]:  # æ¯ä¸ªæŸ¥è¯¢æœ€å¤šæ£€æŸ¥ 15 ä»¶
                if cnt >= MAX_PER_QUERY:
                    break
                try:
                    obj = make_request(session, f"{base}/objects/{oid}").json()
                except Exception:
                    continue

                img_url = obj.get("primaryImage", "")
                if not img_url or img_url in existing_urls:
                    continue
                # ä»…ä¸‹è½½å…¬å¼€é¢†åŸŸçš„
                if not obj.get("isPublicDomain", False):
                    continue

                title = obj.get("title", "") or obj.get("objectName", "")
                period = obj.get("period", "") or obj.get("dynasty", "")
                artist = obj.get("artistDisplayName", "")

                h = md5(img_url.encode()).hexdigest()[:8]
                fn = f"met_{idx:03d}_{sanitize_fn((title or q)[:25])}_{h}.jpg"

                print(f"    -> {fn}")
                if download(session, img_url, IMAGE_DIR / fn,
                            referer="https://www.metmuseum.org/"):
                    entries.append({
                        "filename": fn,
                        "title": title,
                        "description": f"{period} {artist}".strip(),
                        "categories": [obj.get("department", ""), period],
                        "original_url": img_url,
                        "source": "The Metropolitan Museum of Art",
                        "era": period,
                    })
                    existing_urls.add(img_url)
                    idx += 1
                    cnt += 1

                time.sleep(DOWNLOAD_DELAY)
            print(f"    +{cnt}")
        except Exception as e:
            print(f"    è·å–å¤±è´¥: {str(e)[:60]}")

    return entries, idx


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  æ•°æ®æº 4: èŠåŠ å“¥è‰ºæœ¯åšç‰©é¦† Open Access APIï¼ˆå…è´¹ï¼‰
#  https://api.artic.edu/docs/
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ARTIC_QUERIES = [
    "Song dynasty",
    "Southern Song",
    "Chinese landscape painting",
    "Chinese ceramics Song",
    "Chinese calligraphy",
]

def scrape_artic(session, existing_urls, start_idx):
    print("\n" + "=" * 55)
    print("  [æ•°æ®æº 4] èŠåŠ å“¥è‰ºæœ¯åšç‰©é¦† Open Access")
    print("=" * 55)
    entries, idx = [], start_idx
    base = "https://api.artic.edu/api/v1"
    iiif_base = "https://www.artic.edu/iiif/2"

    for qi, q in enumerate(ARTIC_QUERIES):
        print(f"\n  [{qi+1}/{len(ARTIC_QUERIES)}] {q}")
        try:
            resp = make_request(session, f"{base}/artworks/search",
                                params={"q": q, "limit": 15,
                                        "fields": "id,title,date_display,artist_display,"
                                                   "image_id,department_title,is_public_domain"})
            data = resp.json().get("data", [])
            if not data:
                print("    æ— ç»“æœ"); continue

            cnt = 0
            for item in data:
                if cnt >= MAX_PER_QUERY:
                    break
                image_id = item.get("image_id")
                if not image_id or not item.get("is_public_domain", False):
                    continue

                img_url = f"{iiif_base}/{image_id}/full/843,/0/default.jpg"
                if img_url in existing_urls:
                    continue

                title = item.get("title", "")
                period = item.get("date_display", "")
                artist = item.get("artist_display", "")

                h = md5(img_url.encode()).hexdigest()[:8]
                fn = f"artic_{idx:03d}_{sanitize_fn((title or q)[:25])}_{h}.jpg"

                print(f"    -> {fn}")
                if download(session, img_url, IMAGE_DIR / fn,
                            referer="https://www.artic.edu/"):
                    entries.append({
                        "filename": fn,
                        "title": title,
                        "description": f"{period} {artist}".strip(),
                        "categories": [item.get("department_title", ""), period],
                        "original_url": img_url,
                        "source": "Art Institute of Chicago",
                        "era": period,
                    })
                    existing_urls.add(img_url)
                    idx += 1
                    cnt += 1

                time.sleep(DOWNLOAD_DELAY)
            print(f"    +{cnt}")
        except Exception as e:
            print(f"    è·å–å¤±è´¥: {str(e)[:60]}")

    return entries, idx


def main():
    IMAGE_DIR.mkdir(parents=True, exist_ok=True)
    METADATA_FILE.parent.mkdir(parents=True, exist_ok=True)

    # ç¡®è®¤ requests åº“
    session = get_session()

    # åŠ è½½å·²æœ‰è®°å½•ï¼Œæ–­ç‚¹ç»­çˆ¬
    existing_urls = set()
    existing_count = 0
    if METADATA_FILE.exists():
        with open(METADATA_FILE, "r", encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    try:
                        entry = json.loads(line.strip())
                        existing_urls.add(entry.get("original_url", ""))
                        existing_count += 1
                    except: pass
    
    print(f"ğŸ’ å·²æœ‰ {existing_count} æ¡è®°å½•ï¼Œæ–­ç‚¹ç»­çˆ¬\n")
    idx = existing_count
    all_new = []

    # 1. ç™¾åº¦å›¾ç‰‡ï¼ˆæœ€ç¨³ã€æœ€å¿«ï¼‰
    new_entries, idx = scrape_baidu(session, existing_urls, idx)
    all_new.extend(new_entries)
    
    # 2. Wikimedia Commons
    new_entries, idx = scrape_wikimedia(session, existing_urls, idx)
    all_new.extend(new_entries)

    # 3. å¤§éƒ½ä¼šè‰ºæœ¯åšç‰©é¦†ï¼ˆé«˜è´¨é‡å…¬å¼€é¢†åŸŸè—å“ï¼‰
    new_entries, idx = scrape_met_museum(session, existing_urls, idx)
    all_new.extend(new_entries)

    # 4. èŠåŠ å“¥è‰ºæœ¯åšç‰©é¦†ï¼ˆé«˜è´¨é‡ IIIF å›¾åƒï¼‰
    new_entries, idx = scrape_artic(session, existing_urls, idx)
    all_new.extend(new_entries)

    # ä¿å­˜
    if all_new:
        with open(METADATA_FILE, "a", encoding="utf-8") as f:
            for entry in all_new:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")

    total = existing_count + len(all_new)
    print(f"\n{'=' * 55}")
    print(f"ğŸ‰ å®Œæˆ! æœ¬æ¬¡æ–°å¢ {len(all_new)} å¼ ï¼Œç´¯è®¡ {total} å¼ ")
    print(f"  ğŸ“‚ å›¾ç‰‡ç›®å½•: {IMAGE_DIR.resolve()}")
    print(f"  ğŸ“ å…ƒæ•°æ®: {METADATA_FILE.resolve()}")
    print(f"{'=' * 55}")

    if total >= 30:
        print("\nâœ… å›¾ç‰‡æ•°é‡è¾¾æ ‡ï¼ç°åœ¨ä½ å¯ä»¥è¿è¡Œ VLM æ ‡æ³¨è„šæœ¬äº†ï¼š")
        print("   python scripts/auto_annotate.py")


if __name__ == "__main__":
    main()

