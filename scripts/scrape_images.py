# -*- coding: utf-8 -*-
"""
å—å®‹æ–‡åŒ–å›¾ç‰‡å¢å¼ºç‰ˆå¤šæ¸ é“çˆ¬è™« (ä½¿ç”¨ requests)

ä¾èµ–:
    pip install requests
    conda install requests
"""

import os
import re
import json
import time
import random
from pathlib import Path
from hashlib import md5
import argparse

try:
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
except ImportError:
    print("âŒ ç¼ºå°‘ requests åº“ï¼Œè¯·å…ˆè¿è¡Œ: pip install requests")
    import sys
    sys.exit(1)

# ============ é…ç½® ============
IMAGE_DIR = Path("data/images")
METADATA_FILE = Path("data/image_metadata.jsonl")

# éšæœº User-Agent åº“ï¼Œé¿å…è¢«è¯†åˆ«ä¸ºå›ºå®šçˆ¬è™«
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
]

MAX_PER_QUERY = 5          # æ¯ä¸ªå…³é”®è¯ä¸‹è½½æ•°é‡
DOWNLOAD_DELAY = 1.0       # ä¸‹è½½é—´éš”
MIN_IMAGE_SIZE = 20_000    # 20KB


def get_session():
    """åˆ›å»ºä¸€ä¸ªå¸¦æœ‰è‡ªåŠ¨é‡è¯•æœºåˆ¶çš„ Requests Session"""
    session = requests.Session()
    # è®¾ç½®é‡è¯•ç­–ç•¥ï¼šé‡åˆ° 429(é™æµ), 500, 502, 503, 504 ä¼šè‡ªåŠ¨é€€é¿å¹¶é‡è¯•
    retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
    adapter = HTTPAdapter(max_retries=retries)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    # ç¦ç”¨ SSL éªŒè¯è­¦å‘Š
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    return session


def make_request(session, url, headers=None, **kwargs):
    """é€šç”¨çš„ GET è¯·æ±‚ï¼Œå¸¦æœ‰éšæœº UA"""
    hdrs = {
        "User-Agent": random.choice(USER_AGENTS),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    }
    if headers:
        hdrs.update(headers)
    
    # ç¦ç”¨ SSL éªŒè¯ä»¥åº”å¯¹éƒ¨åˆ†å›½å†…å›¾åºŠè¯ä¹¦é—®é¢˜
    kwargs.setdefault("verify", False)
    kwargs.setdefault("timeout", 15)
    
    return session.get(url, headers=hdrs, **kwargs)


def sanitize_fn(name):
    """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
    name = re.sub(r'[<>:"/\\|?*\x00-\x1f]', '_', name)
    name = re.sub(r'\s+', '_', name)
    return name[:80]


def is_image_data(data):
    """é€šè¿‡æ–‡ä»¶å¤´ç­¾ååˆ¤æ–­æ˜¯å¦çœŸçš„æ˜¯å›¾ç‰‡"""
    return (data[:3] == b'\xff\xd8\xff' or          # JPEG
            data[:8] == b'\x89PNG\r\n\x1a\n' or     # PNG
            data[:4] == b'RIFF' or                   # WEBP
            data[:4] == b'GIF8')                     # GIF


def download(session, url, save_path, referer=None):
    """ä¸‹è½½å›¾ç‰‡å¹¶ä¿å­˜"""
    try:
        hdrs = {}
        if referer:
            hdrs["Referer"] = referer
        resp = make_request(session, url, headers=hdrs, stream=True)
        resp.raise_for_status()
        
        data = resp.content
        if len(data) < MIN_IMAGE_SIZE:
            print(f"      - è·³è¿‡ (å¤ªå°: {len(data)//1024}KB)")
            return False
        if not is_image_data(data):
            print("      - è·³è¿‡ (éæœ‰æ•ˆå›¾ç‰‡æ ¼å¼)")
            return False
            
        with open(save_path, "wb") as f:
            f.write(data)
        return True
    except Exception as e:
        # åªæ‰“å°ç®€çŸ­é”™è¯¯
        print(f"      x å¤±è´¥: {str(e)[:50]}")
        return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  æ•°æ®æº 1: ç™¾åº¦å›¾ç‰‡ï¼ˆæœ€ç¨³ã€ä¸­æ–‡æœç´¢ä½“éªŒæœ€å¥½ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BAIDU_QUERIES = [
    # --- ç»˜ç”» ---
    "å—å®‹ å±±æ°´ç”» åšç‰©é¦†",
    "é©¬è¿œ è¸æ­Œå›¾ é«˜æ¸…",
    "å¤åœ­ æºªå±±æ¸…è¿œå›¾",
    "æå” ä¸‡å£‘æ¾é£å›¾",
    "åˆ˜æ¾å¹´ å››æ™¯å±±æ°´å›¾",
    "è¥¿æ¹–åæ™¯ å¤ç”»",
    "å®‹ä»£ èŠ±é¸Ÿç”» é«˜æ¸…",
    "é›·å³°å¡” å¤•ç…§ å¤ç”»",
    "å—å®‹ é£ä¿—ç”»",
    "å—å®‹ äººç‰©ç”» ä»•å¥³",
    "å®‹ä»£ ç½—æ±‰å›¾ é‡Šé“ç”»",
    # --- å™¨ç‰© ---
    "å—å®‹å®˜çª‘ ç“·å™¨",
    "é¾™æ³‰é’ç“· å®‹ä»£ é«˜æ¸…",
    "å®‹ä»£ å»ºç› å¤©ç›®é‡‰",
    "å—å®‹ å“¥çª‘ å†°è£‚çº¹",
    "å®‹ä»£ å®šçª‘ ç™½ç“·",
    "å®‹ä»£ é’§çª‘ çª‘å˜",
    "å—å®‹ é‡‘é“¶å™¨ åšç‰©é¦†",
    "å—å®‹ æ¼†å™¨ èºé’¿",
    "å—å®‹ åˆºç»£ ç¼‚ä¸",
    # --- å»ºç­‘ ---
    "å¾·å¯¿å®«é—å€ æ­å·",
    "å…­å’Œå¡” å®‹ä»£",
    "å—å®‹ çš‡åŸ ä¸´å®‰",
    "å®‹åŸ é—å€ ç…§ç‰‡",
    "çµéšå¯º å®‹ä»£",
    "å‡€æ…ˆå¯º æ­å· å—å®‹",
    # --- ä¹¦æ³• ---
    "å®‹ä»£ ä¹¦æ³• çœŸè¿¹",
    "å—å®‹ ç¢‘å¸– æ‹“ç‰‡",
    # --- åœ°å›¾ ---
    "è¥¿æ¹– å¤ä»£ åœ°å›¾",
    "ä¸´å®‰ å®‹ä»£ åœ°å›¾",
    # --- å…¶ä»– ---
    "æ±Ÿå— å¤é•‡ å—å®‹",
    "å®‹ä»£ ç‰å™¨ åšç‰©é¦†",
]

def scrape_baidu(session, existing_urls, start_idx):
    print("\n" + "=" * 55)
    print("  â­ [æ•°æ®æº 1] ç™¾åº¦å›¾ç‰‡æé€Ÿç‰ˆ")
    print("=" * 55)
    entries, idx = [], start_idx

    for qi, q in enumerate(BAIDU_QUERIES):
        print(f"\n  [{qi+1}/{len(BAIDU_QUERIES)}] {q}")

        params = {
            "tn": "resultjson_com", "ipn": "rj", "ct": "201326592",
            "fp": "result", "word": q, "queryWord": q, "cl": "2",
            "lm": "-1", "ie": "utf-8", "oe": "utf-8", "st": "-1",
            "ic": "0", "istype": "2", "pn": "0", "rn": "20",
        }
        
        try:
            url = "https://image.baidu.com/search/acjson?" + "&".join(f"{k}={v}" for k, v in params.items())
            hdrs = {"Referer": "https://image.baidu.com/", "Accept": "application/json"}
            resp = make_request(session, url, headers=hdrs)
            
            if resp.status_code != 200:
                print(f"    æ¥å£è¯·æ±‚å¤±è´¥ ({resp.status_code})"); continue
                
            # å¤„ç†ç™¾åº¦å¶å°”è¿”å›çš„éæ ‡å‡† JSON
            text = resp.text.replace("\\'", "'")
            data = json.loads(text)
        except Exception as e:
            print(f"    API å¤±è´¥: {str(e)[:60]}"); continue

        items = data.get("data", [])
        if not items:
            print("    æ— ç»“æœ")
            continue
            
        cnt = 0
        for item in items:
            if not isinstance(item, dict): continue
            if cnt >= MAX_PER_QUERY: break
            
            # ä½¿ç”¨åŸå›¾ï¼Œå¤‡ç”¨ç¼©ç•¥å›¾
            img_url = item.get("hoverURL") or item.get("middleURL") or item.get("thumbURL") or item.get("objURL", "")
            if not img_url or img_url in existing_urls:
                continue
                
            from_title = item.get("fromPageTitleEnc", "") or item.get("fromPageTitle", "")
            from_title = re.sub(r'<[^>]+>', '', from_title) # æ¸…ç† HTML æ ‡ç­¾
            
            h = md5(img_url.encode()).hexdigest()[:8]
            ext = "jpg" if ".jpg" in img_url.lower() else "png"
            fn = f"baidu_{idx:03d}_{sanitize_fn(q[:20])}_{h}.{ext}"
            
            print(f"    -> {fn}")
            # å¿…é¡»å¸¦ç™¾åº¦çš„ Referer æ‰èƒ½ä¸‹è½½å¤§å›¾
            if download(session, img_url, IMAGE_DIR / fn, referer="https://image.baidu.com/"):
                entries.append({
                    "filename": fn,
                    "title": from_title or q,
                    "description": f"ç™¾åº¦å›¾ç‰‡: {q}",
                    "categories": ["ç™¾åº¦å›¾ç‰‡"],
                    "original_url": img_url,
                    "source": "Baidu Images",
                })
                existing_urls.add(img_url)
                idx += 1
                cnt += 1
                
            time.sleep(DOWNLOAD_DELAY)
            
        print(f"    +{cnt}")
    return entries, idx


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  æ•°æ®æº 2: Wikimedia Commons
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
WIKI_QUERIES = [
    "Southern Song dynasty painting",
    "West Lake Hangzhou",
    "Song dynasty Longquan celadon",
]

def scrape_wikimedia(session, existing_urls, start_idx):
    print("\n" + "=" * 55)
    print("  [æ•°æ®æº 2] Wikimedia Commons")
    print("=" * 55)
    entries, idx = [], start_idx

    for qi, q in enumerate(WIKI_QUERIES):
        print(f"\n  [{qi+1}/{len(WIKI_QUERIES)}] {q}")

        params = {
            "action": "query", "list": "search", "srsearch": q, 
            "srnamespace": "6", "srlimit": "10", "format": "json"
        }
        try:
            url = "https://commons.wikimedia.org/w/api.php?" + "&".join(f"{k}={v}" for k, v in params.items())
            data = make_request(session, url).json()
            results = data.get("query", {}).get("search", [])
            if not results:
                print("    æ— ç»“æœ"); continue
                
            titles = [r["title"] for r in results]
            
            url2 = "https://commons.wikimedia.org/w/api.php"
            params2 = {
                "action": "query", "titles": "|".join(titles),
                "prop": "imageinfo", "iiprop": "url|mime",
                "iiurlwidth": "800", "format": "json"
            }
            info = make_request(session, f"{url2}?{'&'.join(f'{k}={v}' for k, v in params2.items())}").json()
            pages = info.get("query", {}).get("pages", {})
            
            cnt = 0
            for pid, pg in pages.items():
                if pid == "-1" or cnt >= MAX_PER_QUERY: continue
                ii = pg.get("imageinfo", [{}])[0]
                mime = ii.get("mime", "")
                if mime not in ("image/jpeg", "image/png"): continue
                
                # è¯·æ±‚ç¼©ç•¥å›¾ä»¥é¿å… 429
                url = ii.get("thumburl", "") or ii.get("url", "")
                if not url or url in existing_urls: continue
                
                title = pg.get("title", "").replace("File:", "")
                ext = "jpg" if "jpeg" in mime else "png"
                fn = f"wiki_{idx:03d}_{sanitize_fn(title[:30])}.{ext}"
                
                print(f"    -> {fn}")
                if download(session, url, IMAGE_DIR / fn, referer="https://commons.wikimedia.org/"):
                    entries.append({"filename": fn, "title": title, "description": "",
                                    "categories": ["Wiki"], "original_url": url,
                                    "source": "Wikimedia Commons"})
                    existing_urls.add(url); idx += 1; cnt += 1
                time.sleep(DOWNLOAD_DELAY * 2) # Wiki å®¹æ˜“ 429ï¼Œæ…¢ç‚¹çˆ¬
            print(f"    +{cnt}")
        except Exception as e:
            print(f"    è·å–å¤±è´¥: {str(e)[:50]}")
            
    return entries, idx


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  æ•°æ®æº 3: å¤§éƒ½ä¼šè‰ºæœ¯åšç‰©é¦† Open Access APIï¼ˆå…è´¹ï¼‰
#  https://metmuseum.github.io/
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
MET_QUERIES = [
    "Song dynasty painting",
    "Song dynasty ceramics",
    "Song dynasty calligraphy",
    "Chinese landscape painting Song",
    "Song dynasty jade",
    "Song dynasty bronze",
    "Hangzhou",
]

def scrape_met_museum(session, existing_urls, start_idx):
    print("\n" + "=" * 55)
    print("  [æ•°æ®æº 3] å¤§éƒ½ä¼šè‰ºæœ¯åšç‰©é¦† Open Access")
    print("=" * 55)
    entries, idx = [], start_idx
    base = "https://collectionapi.metmuseum.org/public/collection/v1"

    for qi, q in enumerate(MET_QUERIES):
        print(f"\n  [{qi+1}/{len(MET_QUERIES)}] {q}")
        try:
            resp = make_request(session, f"{base}/search?q={q}&hasImages=true")
            data = resp.json()
            obj_ids = data.get("objectIDs", [])
            if not obj_ids:
                print("    æ— ç»“æœ"); continue

            cnt = 0
            for oid in obj_ids[:15]:  # æ¯ä¸ªæŸ¥è¯¢æœ€å¤šæ£€æŸ¥ 15 ä»¶
                if cnt >= MAX_PER_QUERY:
                    break
                try:
                    obj = make_request(session, f"{base}/objects/{oid}").json()
                except Exception:
                    continue

                img_url = obj.get("primaryImage", "")
                if not img_url or img_url in existing_urls:
                    continue
                # ä»…ä¸‹è½½å…¬å¼€é¢†åŸŸçš„
                if not obj.get("isPublicDomain", False):
                    continue

                title = obj.get("title", "") or obj.get("objectName", "")
                period = obj.get("period", "") or obj.get("dynasty", "")
                artist = obj.get("artistDisplayName", "")

                h = md5(img_url.encode()).hexdigest()[:8]
                fn = f"met_{idx:03d}_{sanitize_fn((title or q)[:25])}_{h}.jpg"

                print(f"    -> {fn}")
                if download(session, img_url, IMAGE_DIR / fn,
                            referer="https://www.metmuseum.org/"):
                    entries.append({
                        "filename": fn,
                        "title": title,
                        "description": f"{period} {artist}".strip(),
                        "categories": [obj.get("department", ""), period],
                        "original_url": img_url,
                        "source": "The Metropolitan Museum of Art",
                        "era": period,
                    })
                    existing_urls.add(img_url)
                    idx += 1
                    cnt += 1

                time.sleep(DOWNLOAD_DELAY)
            print(f"    +{cnt}")
        except Exception as e:
            print(f"    è·å–å¤±è´¥: {str(e)[:60]}")

    return entries, idx


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  æ•°æ®æº 4: èŠåŠ å“¥è‰ºæœ¯åšç‰©é¦† Open Access APIï¼ˆå…è´¹ï¼‰
#  https://api.artic.edu/docs/
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ARTIC_QUERIES = [
    "Song dynasty",
    "Southern Song",
    "Chinese landscape painting",
    "Chinese ceramics Song",
    "Chinese calligraphy",
]

def scrape_artic(session, existing_urls, start_idx):
    print("\n" + "=" * 55)
    print("  [æ•°æ®æº 4] èŠåŠ å“¥è‰ºæœ¯åšç‰©é¦† Open Access")
    print("=" * 55)
    entries, idx = [], start_idx
    base = "https://api.artic.edu/api/v1"
    iiif_base = "https://www.artic.edu/iiif/2"

    for qi, q in enumerate(ARTIC_QUERIES):
        print(f"\n  [{qi+1}/{len(ARTIC_QUERIES)}] {q}")
        try:
            resp = make_request(session, f"{base}/artworks/search",
                                params={"q": q, "limit": 15,
                                        "fields": "id,title,date_display,artist_display,"
                                                   "image_id,department_title,is_public_domain"})
            data = resp.json().get("data", [])
            if not data:
                print("    æ— ç»“æœ"); continue

            cnt = 0
            for item in data:
                if cnt >= MAX_PER_QUERY:
                    break
                image_id = item.get("image_id")
                if not image_id or not item.get("is_public_domain", False):
                    continue

                img_url = f"{iiif_base}/{image_id}/full/843,/0/default.jpg"
                if img_url in existing_urls:
                    continue

                title = item.get("title", "")
                period = item.get("date_display", "")
                artist = item.get("artist_display", "")

                h = md5(img_url.encode()).hexdigest()[:8]
                fn = f"artic_{idx:03d}_{sanitize_fn((title or q)[:25])}_{h}.jpg"

                print(f"    -> {fn}")
                if download(session, img_url, IMAGE_DIR / fn,
                            referer="https://www.artic.edu/"):
                    entries.append({
                        "filename": fn,
                        "title": title,
                        "description": f"{period} {artist}".strip(),
                        "categories": [item.get("department_title", ""), period],
                        "original_url": img_url,
                        "source": "Art Institute of Chicago",
                        "era": period,
                    })
                    existing_urls.add(img_url)
                    idx += 1
                    cnt += 1

                time.sleep(DOWNLOAD_DELAY)
            print(f"    +{cnt}")
        except Exception as e:
            print(f"    è·å–å¤±è´¥: {str(e)[:60]}")

    return entries, idx


def main():
    IMAGE_DIR.mkdir(parents=True, exist_ok=True)
    METADATA_FILE.parent.mkdir(parents=True, exist_ok=True)

    # ç¡®è®¤ requests åº“
    session = get_session()

    # åŠ è½½å·²æœ‰è®°å½•ï¼Œæ–­ç‚¹ç»­çˆ¬
    existing_urls = set()
    existing_count = 0
    if METADATA_FILE.exists():
        with open(METADATA_FILE, "r", encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    try:
                        entry = json.loads(line.strip())
                        existing_urls.add(entry.get("original_url", ""))
                        existing_count += 1
                    except: pass
    
    print(f"ğŸ’ å·²æœ‰ {existing_count} æ¡è®°å½•ï¼Œæ–­ç‚¹ç»­çˆ¬\n")
    idx = existing_count
    all_new = []

    # 1. ç™¾åº¦å›¾ç‰‡ï¼ˆæœ€ç¨³ã€æœ€å¿«ï¼‰
    new_entries, idx = scrape_baidu(session, existing_urls, idx)
    all_new.extend(new_entries)
    
    # 2. Wikimedia Commons
    new_entries, idx = scrape_wikimedia(session, existing_urls, idx)
    all_new.extend(new_entries)

    # 3. å¤§éƒ½ä¼šè‰ºæœ¯åšç‰©é¦†ï¼ˆé«˜è´¨é‡å…¬å¼€é¢†åŸŸè—å“ï¼‰
    new_entries, idx = scrape_met_museum(session, existing_urls, idx)
    all_new.extend(new_entries)

    # 4. èŠåŠ å“¥è‰ºæœ¯åšç‰©é¦†ï¼ˆé«˜è´¨é‡ IIIF å›¾åƒï¼‰
    new_entries, idx = scrape_artic(session, existing_urls, idx)
    all_new.extend(new_entries)

    # ä¿å­˜
    if all_new:
        with open(METADATA_FILE, "a", encoding="utf-8") as f:
            for entry in all_new:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")

    total = existing_count + len(all_new)
    print(f"\n{'=' * 55}")
    print(f"ğŸ‰ å®Œæˆ! æœ¬æ¬¡æ–°å¢ {len(all_new)} å¼ ï¼Œç´¯è®¡ {total} å¼ ")
    print(f"  ğŸ“‚ å›¾ç‰‡ç›®å½•: {IMAGE_DIR.resolve()}")
    print(f"  ğŸ“ å…ƒæ•°æ®: {METADATA_FILE.resolve()}")
    print(f"{'=' * 55}")

    if total >= 30:
        print("\nâœ… å›¾ç‰‡æ•°é‡è¾¾æ ‡ï¼ç°åœ¨ä½ å¯ä»¥è¿è¡Œ VLM æ ‡æ³¨è„šæœ¬äº†ï¼š")
        print("   python scripts/auto_annotate.py")


if __name__ == "__main__":
    main()

