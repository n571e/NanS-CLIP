# -*- coding: utf-8 -*-
"""
ä» Wikimedia Commons è‡ªåŠ¨çˆ¬å–å—å®‹ç›¸å…³å›¾ç‰‡ + å…ƒæ•°æ®ã€‚

ä½¿ç”¨æ–¹æ³•:
    conda activate pytorch
    cd d:/Desktop/CV/CLIP/NanS-CLIP
    python scripts/scrape_wikimedia.py

è¾“å‡º:
    data/images/          ä¸‹è½½çš„å›¾ç‰‡
    data/image_metadata.jsonl   æ¯å¼ å›¾çš„å…ƒæ•°æ®ï¼ˆæ ‡é¢˜ã€æè¿°ã€æ¥æºç­‰ï¼‰
"""

import os
import json
import time
import urllib.request
import urllib.parse
from pathlib import Path

# ============ é…ç½® ============
IMAGE_DIR = Path("data/images")
METADATA_FILE = Path("data/image_metadata.jsonl")
USER_AGENT = "NanS-CLIP/1.0 (Academic research; Song Dynasty culture retrieval)"

# æœç´¢å…³é”®è¯åˆ—è¡¨ï¼ˆä¸­è‹±åŒè¯­ï¼Œå°½å¯èƒ½è¦†ç›–å—å®‹æ–‡åŒ–ä¸»é¢˜ï¼‰
SEARCH_QUERIES = [
    # ===== ç»˜ç”»ç±» =====
    "Southern Song dynasty painting",
    "Song dynasty scroll painting",
    "Song dynasty landscape painting",
    "Song dynasty silk painting",
    "Chinese landscape painting Song",
    "Song dynasty court painting",
    "Song dynasty ink wash painting",
    "Ma Yuan painting",          # é©¬è¿œï¼ˆå—å®‹å››å¤§å®¶ï¼‰
    "Xia Gui painting",         # å¤åœ­ï¼ˆå—å®‹å››å¤§å®¶ï¼‰
    "Li Tang painting",         # æå”
    "Liu Songnian painting",    # åˆ˜æ¾å¹´
    "Song dynasty fan painting",
    "Song dynasty album leaf",
    # ===== äººç‰©ç”» & é£ä¿— =====
    "Song dynasty figure painting",
    "Song dynasty genre painting",
    "Song dynasty Buddhist art",
    "Song dynasty portrait",
    "Song dynasty ladies painting",
    "Song dynasty market scene",
    "Song dynasty festival painting",
    # ===== å™¨ç‰© & å·¥è‰º =====
    "Song dynasty ceramics porcelain",
    "Song dynasty Longquan celadon",
    "Song dynasty Guan ware",
    "Song dynasty Ge ware",
    "Song dynasty Jun ware",
    "Song dynasty Ding ware",
    "Song dynasty lacquerware",
    "Song dynasty jade",
    "Song dynasty bronze mirror",
    "Song dynasty gold silver",
    # ===== å»ºç­‘ & é—å€ =====
    "Song dynasty architecture",
    "Hangzhou historical site",
    "West Lake Hangzhou",
    "Liuhe Pagoda Hangzhou",
    "Leifeng Pagoda",
    "Lin'an Southern Song",
    "Song dynasty temple pagoda",
    "Deshou Palace Hangzhou",
    # ===== ä¹¦æ³• & ç¢‘å¸– =====
    "Song dynasty calligraphy",
    "Song dynasty inscription rubbing",
    "Song dynasty stele",
    # ===== åœ°å›¾ =====
    "Song dynasty map",
    "Lin'an ancient map",
    # ===== ä¸­æ–‡æœç´¢ =====
    "å—å®‹ ç»˜ç”»",
    "è¥¿æ¹– å¤ç”»",
    "å®‹ä»£ é’ç“·",
    "ä¸´å®‰ å—å®‹",
    "å…­å’Œå¡”",
    "å—å®‹ äººç‰©ç”»",
    "å®‹ä»£ ä¹¦æ³•",
    "å—å®‹ å·¥è‰ºç¾æœ¯",
    "å¾·å¯¿å®«",
    "é›·å³°å¡”",
]

# æ¯ä¸ªæŸ¥è¯¢æœ€å¤šä¸‹è½½çš„å›¾ç‰‡æ•°
MAX_PER_QUERY = 10
# æ€»å…±æœ€å¤šä¸‹è½½çš„å›¾ç‰‡æ•°
MAX_TOTAL = 300
# æœ€å°å›¾ç‰‡å°ºå¯¸ï¼ˆè·³è¿‡å¤ªå°çš„ç¼©ç•¥å›¾ï¼‰
MIN_IMAGE_SIZE = 50_000  # 50KB


def wiki_api_request(params: dict) -> dict:
    """è°ƒç”¨ Wikimedia Commons API"""
    base_url = "https://commons.wikimedia.org/w/api.php"
    params["format"] = "json"
    url = base_url + "?" + urllib.parse.urlencode(params)
    req = urllib.request.Request(url, headers={"User-Agent": USER_AGENT})
    try:
        with urllib.request.urlopen(req, timeout=15) as resp:
            return json.loads(resp.read().decode("utf-8"))
    except Exception as e:
        print(f"  âš ï¸ API è¯·æ±‚å¤±è´¥: {e}")
        return {}


def search_images(query: str, limit: int = 10) -> list:
    """é€šè¿‡æœç´¢ API æŸ¥æ‰¾æ–‡ä»¶"""
    data = wiki_api_request({
        "action": "query",
        "list": "search",
        "srsearch": query,
        "srnamespace": "6",  # File namespace
        "srlimit": str(limit),
    })
    return data.get("query", {}).get("search", [])


def get_image_info(titles: list) -> dict:
    """è·å–å›¾ç‰‡çš„ä¸‹è½½ URLã€æè¿°ã€åˆ†ç±»ç­‰å…ƒæ•°æ®"""
    if not titles:
        return {}
    data = wiki_api_request({
        "action": "query",
        "titles": "|".join(titles),
        "prop": "imageinfo|categories",
        "iiprop": "url|size|extmetadata|mime",
        "iiurlwidth": "800",  # è¯·æ±‚ 800px å®½çš„ç¼©ç•¥å›¾
        "cllimit": "10",
    })
    return data.get("query", {}).get("pages", {})


def extract_metadata(page: dict) -> dict:
    """ä» API è¿”å›ä¸­æå–ç»“æ„åŒ–å…ƒæ•°æ®"""
    info = page.get("imageinfo", [{}])[0]
    ext = info.get("extmetadata", {})

    # æå–æè¿°ï¼ˆå¯èƒ½æœ‰å¤šä¸ªè¯­è¨€ç‰ˆæœ¬ï¼‰
    description = ext.get("ImageDescription", {}).get("value", "")
    # æ¸…é™¤ HTML æ ‡ç­¾
    import re
    description = re.sub(r'<[^>]+>', '', description).strip()

    # æå–åˆ†ç±»
    categories = [c["title"].replace("Category:", "") for c in page.get("categories", [])]

    return {
        "title": page.get("title", "").replace("File:", ""),
        "description": description[:500],  # é™åˆ¶é•¿åº¦
        "categories": categories[:10],
        "url": info.get("thumburl") or info.get("url", ""),
        "original_url": info.get("url", ""),
        "width": info.get("width", 0),
        "height": info.get("height", 0),
        "size": info.get("size", 0),
        "mime": info.get("mime", ""),
        "source": "Wikimedia Commons",
    }


def download_image(url: str, save_path: Path) -> bool:
    """ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°"""
    try:
        req = urllib.request.Request(url, headers={"User-Agent": USER_AGENT})
        with urllib.request.urlopen(req, timeout=30) as resp:
            data = resp.read()
            if len(data) < MIN_IMAGE_SIZE:
                print(f"  â­ï¸ è·³è¿‡ï¼ˆå¤ªå° {len(data)//1024}KBï¼‰: {save_path.name}")
                return False
            with open(save_path, "wb") as f:
                f.write(data)
            return True
    except Exception as e:
        print(f"  âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False


def sanitize_filename(name: str) -> str:
    """å°†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿"""
    import re
    name = re.sub(r'[<>:"/\\|?*]', '_', name)
    name = re.sub(r'\s+', '_', name)
    return name[:100]  # é™åˆ¶é•¿åº¦


def main():
    IMAGE_DIR.mkdir(parents=True, exist_ok=True)
    METADATA_FILE.parent.mkdir(parents=True, exist_ok=True)

    # åŠ è½½å·²æœ‰çš„ metadataï¼ˆæ”¯æŒæ–­ç‚¹ç»­çˆ¬ï¼‰
    existing_urls = set()
    existing_entries = []
    if METADATA_FILE.exists():
        with open(METADATA_FILE, "r", encoding="utf-8") as f:
            for line in f:
                entry = json.loads(line.strip())
                existing_urls.add(entry.get("original_url", ""))
                existing_entries.append(entry)
        print(f"ğŸ“‚ å·²æœ‰ {len(existing_entries)} æ¡è®°å½•ï¼Œæ–­ç‚¹ç»­çˆ¬")

    downloaded_count = len(existing_entries)
    new_entries = []

    print(f"ğŸ” å¼€å§‹æœç´¢ï¼Œç›®æ ‡: {MAX_TOTAL} å¼ å›¾ç‰‡\n")

    for qi, query in enumerate(SEARCH_QUERIES):
        if downloaded_count >= MAX_TOTAL:
            break

        print(f"[{qi+1}/{len(SEARCH_QUERIES)}] æœç´¢: {query}")

        # Step 1: æœç´¢
        results = search_images(query, limit=MAX_PER_QUERY + 5)  # å¤šå–ä¸€äº›ï¼Œæœ‰äº›ä¼šè¢«è·³è¿‡
        if not results:
            print("  æ²¡æœ‰ç»“æœ")
            continue

        # Step 2: è·å–è¯¦ç»†ä¿¡æ¯ï¼ˆæ‰¹é‡ API è¯·æ±‚ï¼‰
        titles = [r["title"] for r in results]
        pages = get_image_info(titles)
        if not pages:
            continue

        # Step 3: é€ä¸ªä¸‹è½½
        count_this_query = 0
        for page_id, page in pages.items():
            if page_id == "-1" or downloaded_count >= MAX_TOTAL or count_this_query >= MAX_PER_QUERY:
                continue

            meta = extract_metadata(page)

            # è·³è¿‡éå›¾ç‰‡ï¼ˆsvg, pdf ç­‰ï¼‰
            if meta["mime"] not in ("image/jpeg", "image/png", "image/webp", "image/tiff"):
                continue

            # è·³è¿‡å·²ä¸‹è½½
            if meta["original_url"] in existing_urls:
                continue

            # ç¡®å®šæ–‡ä»¶å
            ext = meta["mime"].split("/")[-1]
            if ext == "jpeg":
                ext = "jpg"
            filename = f"wiki_{downloaded_count:03d}_{sanitize_filename(meta['title'][:50])}.{ext}"
            save_path = IMAGE_DIR / filename

            # ä¸‹è½½
            url = meta["url"] or meta["original_url"]
            if not url:
                continue

            print(f"  â¬‡ï¸ ä¸‹è½½: {filename}")
            if download_image(url, save_path):
                meta["filename"] = filename
                meta.pop("url", None)  # ä¸éœ€è¦ç¼©ç•¥å›¾ url
                new_entries.append(meta)
                existing_urls.add(meta["original_url"])
                downloaded_count += 1
                count_this_query += 1

        print(f"  âœ… æœ¬è½®ä¸‹è½½ {count_this_query} å¼ \n")

        # ç¤¼è²Œçˆ¬å–ï¼šæ¯è½®é—´éš” 1 ç§’
        time.sleep(1)

    # ä¿å­˜ metadataï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
    with open(METADATA_FILE, "a", encoding="utf-8") as f:
        for entry in new_entries:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")

    total = len(existing_entries) + len(new_entries)
    print(f"ğŸ‰ å®Œæˆï¼å…± {total} å¼ å›¾ç‰‡")
    print(f"   å›¾ç‰‡ç›®å½•: {IMAGE_DIR.resolve()}")
    print(f"   å…ƒæ•°æ®æ–‡ä»¶: {METADATA_FILE.resolve()}")

    if total < 30:
        print(f"\nâš ï¸ å›¾ç‰‡æ•°é‡åå°‘ï¼ˆ{total}<30ï¼‰ï¼Œå»ºè®®ï¼š")
        print("   1. æ£€æŸ¥ç½‘ç»œæ˜¯å¦éœ€è¦ä»£ç†")
        print("   2. æ‰‹åŠ¨ä»æ•…å®«æ•°å­—æ–‡ç‰©åº“ (digicol.dpm.org.cn) è¡¥å……å‡ å¼ ")
        print("   3. æ‰‹åŠ¨åœ¨ image_metadata.jsonl ä¸­æ·»åŠ å¯¹åº”è®°å½•")


if __name__ == "__main__":
    main()
