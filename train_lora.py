# -*- coding: utf-8 -*-
"""
Chinese-CLIP + LoRA å•å¡è®­ç»ƒè„šæœ¬ï¼ˆä¸ä¾èµ– DDP åˆ†å¸ƒå¼ï¼‰ã€‚

ä½¿ç”¨æ–¹æ³•:
    conda activate pytorch
    cd d:/Desktop/CV/CLIP/NanS-CLIP

    # ç¡®ä¿å·²å®Œæˆæ•°æ®é›†æ„å»ºï¼ˆLMDB æ ¼å¼ï¼‰
    # python scripts/build_dataset.py
    # python cn_clip/preprocess/build_lmdb_dataset.py --data_dir ../clip_data/datasets/SongDynasty --splits train,valid

    # å¼€å§‹è®­ç»ƒ
    python train_lora.py

    # è‡ªå®šä¹‰å‚æ•°
    python train_lora.py --epochs 50 --lr 2e-4 --rank 8 --batch_size 4
"""

import math
import os
import sys
import json
import argparse
import time
import logging
from pathlib import Path

# ============ æ—¥å¿—é…ç½® ============
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
import lmdb
import pickle

from cn_clip.clip import load_from_name, tokenize
from cn_clip.clip.lora import inject_lora, get_lora_state_dict


# ============ æ•°æ®é›†ç±» ============
class LMDBDataset(Dataset):
    """ä» LMDB è¯»å–å›¾æ–‡å¯¹ï¼ˆå…¼å®¹ Chinese-CLIP çš„ LMDB æ ¼å¼ï¼‰"""

    def __init__(self, lmdb_dir: str, preprocess, max_txt_length: int = 52):
        self.lmdb_dir = lmdb_dir
        self.preprocess = preprocess
        self.max_txt_length = max_txt_length

        # æ‰“å¼€ pairs LMDB
        pairs_path = os.path.join(lmdb_dir, "pairs")
        self.env_pairs = lmdb.open(pairs_path, readonly=True, lock=False)
        with self.env_pairs.begin() as txn:
            self.num_samples = int(txn.get(b"num_samples").decode("utf-8"))

        # æ‰“å¼€ imgs LMDB
        imgs_path = os.path.join(lmdb_dir, "imgs")
        self.env_imgs = lmdb.open(imgs_path, readonly=True, lock=False)

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        import base64
        from io import BytesIO
        from PIL import Image

        # è¯»å– (image_id, text_id, text) å¯¹
        with self.env_pairs.begin() as txn:
            data = pickle.loads(txn.get(str(idx).encode("utf-8")))
        image_id, text_id, text = data

        # è¯»å–å›¾ç‰‡ base64
        with self.env_imgs.begin() as txn:
            b64 = txn.get(str(image_id).encode("utf-8")).decode("utf-8")

        # è§£ç å›¾ç‰‡
        img_bytes = base64.b64decode(b64)
        image = Image.open(BytesIO(img_bytes)).convert("RGB")
        image = self.preprocess(image)

        # Tokenize æ–‡æœ¬
        text_tokens = tokenize([text], context_length=self.max_txt_length)[0]

        return image, text_tokens


def contrastive_loss(image_features, text_features, logit_scale, label_smoothing=0.05):
    """InfoNCE å¯¹æ¯”æŸå¤±ï¼Œå¢åŠ  Label Smoothing ç¼“è§£å°æ•°æ®é›†è¿‡æ‹Ÿåˆ"""
    image_features = F.normalize(image_features, dim=-1)
    text_features = F.normalize(text_features, dim=-1)

    logits = logit_scale * image_features @ text_features.T
    batch_size = logits.shape[0]
    device = logits.device

    # ä½¿ç”¨ F.cross_entropy çš„ label_smoothing å‚æ•° (è¦æ±‚ PyTorch >= 1.10)
    labels = torch.arange(batch_size, device=device)
    loss_i2t = F.cross_entropy(logits, labels, label_smoothing=label_smoothing)
    loss_t2i = F.cross_entropy(logits.T, labels, label_smoothing=label_smoothing)

    return (loss_i2t + loss_t2i) / 2


def main():
    parser = argparse.ArgumentParser(description="Chinese-CLIP + LoRA è®­ç»ƒ")
    parser.add_argument("--data_dir", type=str, default="../clip_data/datasets/SongDynasty/lmdb/train")
    parser.add_argument("--val_dir", type=str, default="../clip_data/datasets/SongDynasty/lmdb/valid")
    parser.add_argument("--pretrained", type=str, default="../clip_data/pretrained_weights/")
    parser.add_argument("--output_dir", type=str, default="../clip_data/experiments/lora_song")
    parser.add_argument("--rank", type=int, default=8, help="LoRA rank")
    parser.add_argument("--warmup_ratio", type=float, default=0.1, help="Warmup æ­¥æ•°å æ€»æ­¥æ•°æ¯”ä¾‹")
    parser.add_argument("--alpha", type=float, default=16.0, help="LoRA alpha")
    parser.add_argument("--batch_size", type=int, default=8)
    parser.add_argument("--accum_freq", type=int, default=4, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œç­‰æ•ˆ batch = batch_size * accum_freq")
    parser.add_argument("--lr", type=float, default=5e-5)   
    parser.add_argument("--wd", type=float, default=0.05)
    parser.add_argument("--epochs", type=int, default=30)
    parser.add_argument("--fp16", action="store_true", default=True)
    parser.add_argument("--text_only", action="store_true", default=False, help="æ˜¯å¦ä»…å¾®è°ƒæ–‡æœ¬ç¼–ç å™¨ï¼ˆé˜²æ­¢ç¾éš¾æ€§é—å¿˜ï¼‰")
    parser.add_argument("--save_every", type=int, default=5, help="æ¯ N ä¸ª epoch ä¿å­˜ä¸€æ¬¡")
    args = parser.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"ğŸ”§ è®¾å¤‡: {device}")

    # 1. åŠ è½½æ¨¡å‹
    logger.info("ğŸ“¦ åŠ è½½ Chinese-CLIP ViT-B-16...")
    model, preprocess = load_from_name("ViT-B-16", device=device, download_root=args.pretrained)
    
    # å¼ºåˆ¶å°†æ¨¡å‹ä»åŠç²¾åº¦ï¼ˆFP16ï¼‰è½¬æ¢ä¸ºå•ç²¾åº¦ï¼ˆFP32ï¼‰ï¼Œå…¼å®¹ PyTorch åŸç”Ÿ AMP
    model.float()

    # 2. æ³¨å…¥ LoRA
    logger.info(f"ğŸ”— æ³¨å…¥ LoRA (rank={args.rank}, alpha={args.alpha}, text_only={args.text_only})...")
    lora_params = inject_lora(model, rank=args.rank, alpha=args.alpha, text_only=args.text_only)

    # 3. å†»ç»“é LoRA å‚æ•°ï¼ˆå¹¶ç¡®ä¿ logit_scale ä¸è¢«æ›´æ–°é¿å…æåº¦è‡ªä¿¡è¿‡æ‹Ÿåˆï¼‰
    for name, param in model.named_parameters():
        if "lora_" not in name:
            param.requires_grad = False
    
    if hasattr(model, "logit_scale"):
        model.logit_scale.requires_grad = False

    total = sum(p.numel() for p in model.parameters())
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"ğŸ“Š æ€»å‚æ•°: {total:,} | å¯è®­ç»ƒ(LoRA): {trainable:,} | å æ¯”: {trainable/total*100:.2f}%")

    # 4. æ•°æ®é›†
    logger.info("ğŸ“‚ åŠ è½½æ•°æ®é›†...")
    train_dataset = LMDBDataset(args.data_dir, preprocess)
    logger.info(f"è®­ç»ƒé›†: {len(train_dataset)} å¯¹")

    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=0,  # Windows ä¸‹å»ºè®®è®¾ä¸º 0
        drop_last=True,
        pin_memory=True,
    )

    val_dataset = None
    if os.path.isdir(args.val_dir):
        val_dataset = LMDBDataset(args.val_dir, preprocess)
        val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0)
        logger.info(f"éªŒè¯é›†: {len(val_dataset)} å¯¹")

    # 5. ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        [p for p in model.parameters() if p.requires_grad],
        lr=args.lr,
        weight_decay=args.wd,
    )

    # 6. å­¦ä¹ ç‡è°ƒåº¦ï¼šLinear Warmup + Cosine Annealing
    total_steps = args.epochs * (len(train_dataset) // args.batch_size + 1)
    warmup_steps = int(total_steps * args.warmup_ratio)

    def lr_lambda(current_step):
        if current_step < warmup_steps:
            return float(current_step) / float(max(1, warmup_steps))
        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))
        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))

    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    logger.info(f"ğŸ“ˆ å­¦ä¹ ç‡è°ƒåº¦: Warmup {warmup_steps} steps â†’ Cosine decay, æ€» {total_steps} steps")

    # 7. æ··åˆç²¾åº¦
    scaler = torch.amp.GradScaler("cuda") if args.fp16 and device == "cuda" else None

    # 8. è¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # 9. è®­ç»ƒå¾ªç¯
    # å…³é”®ï¼šå¯¹æ¯”å­¦ä¹ çš„è´Ÿæ ·æœ¬æ•°é‡ = batch_size - 1
    # å¿…é¡»åœ¨ç´¯ç§¯åçš„å¤§ batch ä¸Šè®¡ç®— contrastive lossï¼Œè€Œä¸æ˜¯æ¯ä¸ª mini-batch ç‹¬ç«‹è®¡ç®—
    effective_batch = args.batch_size * args.accum_freq
    logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼ˆ{args.epochs} epochs, å¯¹æ¯”å­¦ä¹  batch = {effective_batch}ï¼‰")
    best_val_loss = float("inf")

    # åˆå§‹åŒ–è®­ç»ƒæ—¥å¿—
    log_path = output_dir / "training_log.csv"
    with open(log_path, "w", encoding="utf-8") as f:
        f.write("epoch,train_loss,val_loss,lr,is_best\n")
    logger.info(f"ğŸ“ è®­ç»ƒæ—¥å¿—: {log_path}")

    for epoch in range(args.epochs):
        model.train()
        epoch_loss = 0.0
        num_updates = 0
        optimizer.zero_grad()

        # ç´¯ç§¯ç‰¹å¾çš„ç¼“å†²åŒº
        accum_img_feats = []
        accum_txt_feats = []

        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{args.epochs}")
        for step, (images, texts) in enumerate(pbar):
            images = images.to(device)
            texts = texts.to(device)

            # Forward: æå–ç‰¹å¾å¹¶ä¿ç•™è®¡ç®—å›¾
            if scaler is not None:
                with torch.amp.autocast("cuda"):
                    img_f = model.encode_image(images)
                    txt_f = model.encode_text(texts)
            else:
                img_f = model.encode_image(images)
                txt_f = model.encode_text(texts)

            accum_img_feats.append(img_f)
            accum_txt_feats.append(txt_f)

            # æ¯ç´¯ç§¯ accum_freq ä¸ª mini-batchï¼Œæ‹¼æ¥æˆå¤§ batch è®¡ç®— Loss
            if (step + 1) % args.accum_freq == 0:
                all_img = torch.cat(accum_img_feats, dim=0)
                all_txt = torch.cat(accum_txt_feats, dim=0)

                if scaler is not None:
                    with torch.amp.autocast("cuda"):
                        loss = contrastive_loss(all_img, all_txt, model.logit_scale.exp())
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    loss = contrastive_loss(all_img, all_txt, model.logit_scale.exp())
                    loss.backward()
                    optimizer.step()

                optimizer.zero_grad()
                scheduler.step()

                epoch_loss += loss.item()
                num_updates += 1

                # æ¸…ç©ºç´¯ç§¯ç¼“å†²
                accum_img_feats = []
                accum_txt_feats = []

            pbar.set_postfix(loss=f"{epoch_loss/max(num_updates,1):.4f}")

        avg_loss = epoch_loss / max(num_updates, 1)

        # éªŒè¯
        val_msg = ""
        if val_dataset is not None:
            model.eval()
            val_loss = 0.0
            val_batches = 0
            with torch.no_grad():
                for images, texts in val_loader:
                    images, texts = images.to(device), texts.to(device)
                    if scaler is not None:
                        with torch.amp.autocast("cuda"):
                            img_feat = model.encode_image(images)
                            txt_feat = model.encode_text(texts)
                            loss = contrastive_loss(img_feat, txt_feat, model.logit_scale.exp())
                    else:
                        img_feat = model.encode_image(images)
                        txt_feat = model.encode_text(texts)
                        loss = contrastive_loss(img_feat, txt_feat, model.logit_scale.exp())
                    val_loss += loss.item()
                    val_batches += 1
            avg_val = val_loss / max(val_batches, 1)
            val_msg = f" | val_loss: {avg_val:.4f}"

            if avg_val < best_val_loss:
                best_val_loss = avg_val
                save_path = output_dir / "best_lora.pt"
                torch.save(get_lora_state_dict(model), save_path)
                val_msg += " â­ best"

        logger.info(f"Epoch {epoch+1}: train_loss={avg_loss:.4f}{val_msg}")

        # è®°å½•æ—¥å¿—
        current_lr = optimizer.param_groups[0]["lr"]
        avg_val_str = f"{avg_val:.6f}" if val_dataset is not None else ""
        is_best = "â­" if val_msg and "â­" in val_msg else ""
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(f"{epoch+1},{avg_loss:.6f},{avg_val_str},{current_lr:.8f},{is_best}\n")

        # å®šæœŸä¿å­˜
        if (epoch + 1) % args.save_every == 0:
            save_path = output_dir / f"lora_epoch{epoch+1}.pt"
            torch.save(get_lora_state_dict(model), save_path)
            logger.info(f"ğŸ’¾ ä¿å­˜: {save_path}")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = output_dir / "lora_final.pt"
    torch.save(get_lora_state_dict(model), final_path)
    logger.info(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ç»ˆ LoRA æƒé‡: {final_path}")
    logger.info(f"æœ€ä½³éªŒè¯ loss: {best_val_loss:.4f}")


if __name__ == "__main__":
    main()
