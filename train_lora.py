# -*- coding: utf-8 -*-
"""
Chinese-CLIP + LoRA å•å¡è®­ç»ƒè„šæœ¬ï¼ˆä¸ä¾èµ– DDP åˆ†å¸ƒå¼ï¼‰ã€‚

ä½¿ç”¨æ–¹æ³•:
    conda activate pytorch
    cd d:/Desktop/CV/CLIP/NanS-CLIP

    # ç¡®ä¿å·²å®Œæˆæ•°æ®é›†æ„å»ºï¼ˆLMDB æ ¼å¼ï¼‰
    # python scripts/build_dataset.py
    # python cn_clip/preprocess/build_lmdb_dataset.py --data_dir ../clip_data/datasets/SongDynasty --splits train,valid

    # å¼€å§‹è®­ç»ƒ
    python train_lora.py

    # è‡ªå®šä¹‰å‚æ•°
    python train_lora.py --epochs 50 --lr 2e-4 --rank 8 --batch_size 4
"""

import math
import os
import sys
import json
import argparse
import time
from pathlib import Path

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
import lmdb
import pickle

from cn_clip.clip import load_from_name, tokenize
from cn_clip.clip.lora import inject_lora, get_lora_state_dict


# ============ æ•°æ®é›†ç±» ============
class LMDBDataset(Dataset):
    """ä» LMDB è¯»å–å›¾æ–‡å¯¹ï¼ˆå…¼å®¹ Chinese-CLIP çš„ LMDB æ ¼å¼ï¼‰"""

    def __init__(self, lmdb_dir: str, preprocess, max_txt_length: int = 52):
        self.lmdb_dir = lmdb_dir
        self.preprocess = preprocess
        self.max_txt_length = max_txt_length

        # æ‰“å¼€ pairs LMDB
        pairs_path = os.path.join(lmdb_dir, "pairs")
        self.env_pairs = lmdb.open(pairs_path, readonly=True, lock=False)
        with self.env_pairs.begin() as txn:
            self.num_samples = int(txn.get(b"num_samples").decode("utf-8"))

        # æ‰“å¼€ imgs LMDB
        imgs_path = os.path.join(lmdb_dir, "imgs")
        self.env_imgs = lmdb.open(imgs_path, readonly=True, lock=False)

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        import base64
        from io import BytesIO
        from PIL import Image

        # è¯»å– (image_id, text_id, text) å¯¹
        with self.env_pairs.begin() as txn:
            data = pickle.loads(txn.get(str(idx).encode("utf-8")))
        image_id, text_id, text = data

        # è¯»å–å›¾ç‰‡ base64
        with self.env_imgs.begin() as txn:
            b64 = txn.get(str(image_id).encode("utf-8")).decode("utf-8")

        # è§£ç å›¾ç‰‡
        img_bytes = base64.b64decode(b64)
        image = Image.open(BytesIO(img_bytes)).convert("RGB")
        image = self.preprocess(image)

        # Tokenize æ–‡æœ¬
        text_tokens = tokenize([text], context_length=self.max_txt_length)[0]

        return image, text_tokens


def contrastive_loss(image_features, text_features, logit_scale):
    """InfoNCE å¯¹æ¯”æŸå¤±"""
    # å½’ä¸€åŒ–
    image_features = F.normalize(image_features, dim=-1)
    text_features = F.normalize(text_features, dim=-1)

    # ç›¸ä¼¼åº¦çŸ©é˜µ
    logits = logit_scale * image_features @ text_features.T
    labels = torch.arange(len(image_features), device=image_features.device)

    loss_i2t = F.cross_entropy(logits, labels)
    loss_t2i = F.cross_entropy(logits.T, labels)
    return (loss_i2t + loss_t2i) / 2


def main():
    import sys
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    parser = argparse.ArgumentParser(description="Chinese-CLIP + LoRA è®­ç»ƒ")
    parser.add_argument("--data_dir", type=str, default="../clip_data/datasets/SongDynasty/lmdb/train")
    parser.add_argument("--val_dir", type=str, default="../clip_data/datasets/SongDynasty/lmdb/valid")
    parser.add_argument("--pretrained", type=str, default="../clip_data/pretrained_weights/")
    parser.add_argument("--output_dir", type=str, default="../clip_data/experiments/lora_song")
    parser.add_argument("--rank", type=int, default=8, help="LoRA rank")
    parser.add_argument("--warmup_ratio", type=float, default=0.1, help="Warmup æ­¥æ•°å æ€»æ­¥æ•°æ¯”ä¾‹")
    parser.add_argument("--alpha", type=float, default=16.0, help="LoRA alpha")
    parser.add_argument("--batch_size", type=int, default=8)
    parser.add_argument("--accum_freq", type=int, default=4, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œç­‰æ•ˆ batch = batch_size * accum_freq")
    parser.add_argument("--lr", type=float, default=1e-4)
    parser.add_argument("--wd", type=float, default=0.01)
    parser.add_argument("--epochs", type=int, default=30)
    parser.add_argument("--fp16", action="store_true", default=True)
    parser.add_argument("--save_every", type=int, default=5, help="æ¯ N ä¸ª epoch ä¿å­˜ä¸€æ¬¡")
    args = parser.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ”§ è®¾å¤‡: {device}")

    # 1. åŠ è½½æ¨¡å‹
    print("ğŸ“¦ åŠ è½½ Chinese-CLIP ViT-B-16...")
    model, preprocess = load_from_name("ViT-B-16", device=device, download_root=args.pretrained)
    
    # å¼ºåˆ¶å°†æ¨¡å‹ä»åŠç²¾åº¦ï¼ˆFP16ï¼‰è½¬æ¢ä¸ºå•ç²¾åº¦ï¼ˆFP32ï¼‰ï¼Œå…¼å®¹ PyTorch åŸç”Ÿ AMP
    model.float()

    # 2. æ³¨å…¥ LoRA
    print(f"ğŸ”— æ³¨å…¥ LoRA (rank={args.rank}, alpha={args.alpha})...")
    lora_params = inject_lora(model, rank=args.rank, alpha=args.alpha)

    # 3. å†»ç»“é LoRA å‚æ•°
    for name, param in model.named_parameters():
        if "lora_" not in name:
            param.requires_grad = False

    total = sum(p.numel() for p in model.parameters())
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ“Š æ€»å‚æ•°: {total:,} | å¯è®­ç»ƒ(LoRA): {trainable:,} | å æ¯”: {trainable/total*100:.2f}%")

    # 4. æ•°æ®é›†
    print("ğŸ“‚ åŠ è½½æ•°æ®é›†...")
    train_dataset = LMDBDataset(args.data_dir, preprocess)
    print(f"   è®­ç»ƒé›†: {len(train_dataset)} å¯¹")

    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=0,  # Windows ä¸‹å»ºè®®è®¾ä¸º 0
        drop_last=True,
        pin_memory=True,
    )

    val_dataset = None
    if os.path.isdir(args.val_dir):
        val_dataset = LMDBDataset(args.val_dir, preprocess)
        val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0)
        print(f"   éªŒè¯é›†: {len(val_dataset)} å¯¹")

    # 5. ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        [p for p in model.parameters() if p.requires_grad],
        lr=args.lr,
        weight_decay=args.wd,
    )

    # 6. å­¦ä¹ ç‡è°ƒåº¦ï¼šLinear Warmup + Cosine Annealing
    total_steps = args.epochs * (len(train_dataset) // args.batch_size + 1)
    warmup_steps = int(total_steps * args.warmup_ratio)

    def lr_lambda(current_step):
        if current_step < warmup_steps:
            return float(current_step) / float(max(1, warmup_steps))
        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))
        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))

    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    print(f"ğŸ“ˆ å­¦ä¹ ç‡è°ƒåº¦: Warmup {warmup_steps} steps â†’ Cosine decay, æ€» {total_steps} steps")

    # 7. æ··åˆç²¾åº¦
    scaler = torch.amp.GradScaler("cuda") if args.fp16 and device == "cuda" else None

    # 8. è¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # 9. è®­ç»ƒå¾ªç¯
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒï¼ˆ{args.epochs} epochs, ç­‰æ•ˆ batch = {args.batch_size * args.accum_freq}ï¼‰\n")
    best_val_loss = float("inf")

    for epoch in range(args.epochs):
        model.train()
        epoch_loss = 0.0
        num_batches = 0
        optimizer.zero_grad()

        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{args.epochs}")
        for step, (images, texts) in enumerate(pbar):
            images = images.to(device)
            texts = texts.to(device)

            if scaler is not None:
                with torch.amp.autocast("cuda"):
                    image_features = model.encode_image(images)
                    text_features = model.encode_text(texts)
                    loss = contrastive_loss(image_features, text_features, model.logit_scale.exp())
                    loss = loss / args.accum_freq

                scaler.scale(loss).backward()

                if (step + 1) % args.accum_freq == 0:
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad()
                    scheduler.step()
            else:
                image_features = model.encode_image(images)
                text_features = model.encode_text(texts)
                loss = contrastive_loss(image_features, text_features, model.logit_scale.exp())
                loss = loss / args.accum_freq
                loss.backward()

                if (step + 1) % args.accum_freq == 0:
                    optimizer.step()
                    optimizer.zero_grad()
                    scheduler.step()

            epoch_loss += loss.item() * args.accum_freq
            num_batches += 1
            pbar.set_postfix(loss=f"{epoch_loss/num_batches:.4f}")

        avg_loss = epoch_loss / max(num_batches, 1)

        # éªŒè¯
        val_msg = ""
        if val_dataset is not None:
            model.eval()
            val_loss = 0.0
            val_batches = 0
            with torch.no_grad():
                for images, texts in val_loader:
                    images, texts = images.to(device), texts.to(device)
                    if scaler is not None:
                        with torch.amp.autocast("cuda"):
                            img_feat = model.encode_image(images)
                            txt_feat = model.encode_text(texts)
                            loss = contrastive_loss(img_feat, txt_feat, model.logit_scale.exp())
                    else:
                        img_feat = model.encode_image(images)
                        txt_feat = model.encode_text(texts)
                        loss = contrastive_loss(img_feat, txt_feat, model.logit_scale.exp())
                    val_loss += loss.item()
                    val_batches += 1
            avg_val = val_loss / max(val_batches, 1)
            val_msg = f" | val_loss: {avg_val:.4f}"

            if avg_val < best_val_loss:
                best_val_loss = avg_val
                save_path = output_dir / "best_lora.pt"
                torch.save(get_lora_state_dict(model), save_path)
                val_msg += " â­ best"

        print(f"  Epoch {epoch+1}: train_loss={avg_loss:.4f}{val_msg}")

        # å®šæœŸä¿å­˜
        if (epoch + 1) % args.save_every == 0:
            save_path = output_dir / f"lora_epoch{epoch+1}.pt"
            torch.save(get_lora_state_dict(model), save_path)
            print(f"  ğŸ’¾ ä¿å­˜: {save_path}")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = output_dir / "lora_final.pt"
    torch.save(get_lora_state_dict(model), final_path)
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ç»ˆ LoRA æƒé‡: {final_path}")
    print(f"   æœ€ä½³éªŒè¯ loss: {best_val_loss:.4f}")


if __name__ == "__main__":
    main()
